#!/usr/bin/env python
import streamlit as st
import pandas as pd
import altair as alt
import numpy as np
from pathlib import Path
import joblib
import json
import os

from functools import lru_cache
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from modules.data_loader import load_slice, list_cities, list_zones, list_routes
from modules.geo_routes import load_routes_geo
from map_component import map_routes  # custom map component

from modules.model_utils import (
    forecast_gru,
    forecast_rnn,
    forecast_lstm,
    forecast_week_after_last_point,
)
from modules.model_manager import load_model_context

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# =========================
# HCMC: c·∫•u h√¨nh cho travel-time
# =========================

# Chi·ªÅu d√†i x·∫•p x·ªâ c·ªßa t·ª´ng tuy·∫øn (km) ‚Äì b·∫°n c√≥ th·ªÉ ch·ªânh l·∫°i cho s√°t th·ª±c t·∫ø h∆°n
HCMC_ROUTE_LENGTH_KM = {
    "ly_thuong_kiet": 4.3,
    "nguyen_kiem": 3.8,
    "quang_trung": 5.6,
    "nguyen_dinh_chieu": 3.2,
    "le_duc_tho": 7.2,
    "quoc_lo_1a": 51.0,
    "to_hien_thanh": 2.1,
    "truong_chinh": 8.5
}

HCMC_DEFAULT_LENGTH_KM = 4.0          # n·∫øu route ch∆∞a c√≥ trong dict tr√™n
HCMC_FREE_FLOW_SPEED_KMH = 40.0       # t·ªëc ƒë·ªô "tho√°ng" m·∫∑c ƒë·ªãnh trong n·ªôi ƒë√¥

# =====================================================
# H√ÄM T√çNH CH·ªà S·ªê ƒê√ÅNH GI√Å CHUNG CHO UI
# =====================================================

def compute_common_metrics(
    y_true,
    y_pred,
    *,
    task: str = "regression",
    acc_tolerance: float = 0.2,
    threshold: float = 0.5,
) -> dict:
    """
    MSE / RMSE / MAE / SMAPE / Accuracy ‚Äì d√πng cho UI.

    - task="regression": I-94, Fremont, v.v.
        Accuracy = % ƒëi·ªÉm c√≥ sai s·ªë t∆∞∆°ng ƒë·ªëi <= acc_tolerance.
    - task="binary_prob": HCMC congestion.
        Accuracy = accuracy nh·ªã ph√¢n sau khi threshold.
    """
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)

    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)
    if not np.any(mask):
        return {
            "MSE": np.nan,
            "RMSE": np.nan,
            "MAE": np.nan,
            "SMAPE": np.nan,
            "Accuracy": np.nan,
        }

    y_true = y_true[mask]
    y_pred = y_pred[mask]

    diff = y_pred - y_true
    mse = float(np.mean(diff**2))
    rmse = float(np.sqrt(mse))
    mae = float(np.mean(np.abs(diff)))

    denom = np.abs(y_true) + np.abs(y_pred)
    smape = float(
        np.mean(
            2.0 * np.abs(diff) / (denom + 1e-8)
        )
        * 100.0
    )

    if task == "regression":
        rel_err = np.abs(diff) / (np.abs(y_true) + 1e-8)
        acc = float(np.mean(rel_err <= acc_tolerance) * 100.0)
    elif task == "binary_prob":
        y_bin = (y_pred >= threshold).astype(float)
        acc = float(np.mean(y_bin == y_true) * 100.0)
    else:
        acc = np.nan

    return {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "SMAPE": smape,
        "Accuracy": acc,
    }


def get_hcmc_route_length_km(route_id: str) -> float:
    """Tr·∫£ v·ªÅ chi·ªÅu d√†i tuy·∫øn (km), n·∫øu kh√¥ng c√≥ th√¨ d√πng default."""
    return HCMC_ROUTE_LENGTH_KM.get(route_id, HCMC_DEFAULT_LENGTH_KM)


# ARIMA / SARIMA (optional)
try:
    from modules.arima_utils import forecast_arima_for_day
    HAS_ARIMA = True
except Exception:
    forecast_arima_for_day = None
    HAS_ARIMA = False

try:
    from modules.arima_utils import forecast_sarima_for_day
    HAS_SARIMA = True
except Exception:
    forecast_sarima_for_day = None
    HAS_SARIMA = False


@st.cache_resource
def get_model_context(city: str, zone: str | None):
    """
    Cache ModelContext cho m·ªói (city, zone) ƒë·ªÉ tr√°nh load model nhi·ªÅu l·∫ßn.
    """
    return load_model_context(city, zone)


@lru_cache(maxsize=None)
def load_lstm_artifacts_for_family(family_name: str):
    """
    Load LSTM artifacts trong:
        model/<family_name>/

    Tr·∫£ v·ªÅ dict:
      {
        "model", "meta", "scaler",
        "routes", "rid2idx", "dir"
      }
    ho·∫∑c None n·∫øu thi·∫øu file.
    """
    base = Path("model")
    model_dir = base / family_name

    meta_path = model_dir / "lstm_meta.json"
    model_path = model_dir / "traffic_lstm.keras"
    scaler_path = model_dir / "vehicles_scaler.pkl"

    if not (meta_path.exists() and model_path.exists() and scaler_path.exists()):
        print(
            f"[LSTM] Missing artifacts in {model_dir}: "
            f"{meta_path.exists()=}, {model_path.exists()=}, {scaler_path.exists()=}"
        )
        return None

    print(f"[LSTM] Using LSTM model dir: {model_dir}")

    with open(meta_path, "r") as f:
        meta = json.load(f)
    from tensorflow.keras.models import load_model
    model = load_model(model_path)
    scaler = joblib.load(scaler_path)

    routes = list(meta.get("routes", []))
    rid2idx = {rid: i for i, rid in enumerate(routes)}

    return {
        "model": model,
        "meta": meta,
        "scaler": scaler,
        "routes": routes,
        "rid2idx": rid2idx,
        "dir": str(model_dir),
    }


# ======================================================
# HELPER: Forecast 24h cho 1 ng√†y c·ª• th·ªÉ (GRU / RNN / LSTM)
# ======================================================
def forecast_one_day(
    route_id,
    forecast_date: pd.Timestamp,
    city,
    zone,
    ctx,
    seq_model_type: str = "GRU",
):
    """
    Forecast 24h cho 1 ng√†y c·ª• th·ªÉ (00:00 -> 23:00) b·∫±ng GRU / RNN / LSTM,
    d·ª±a tr√™n window history LOOKBACK gi·ªù ngay tr∆∞·ªõc forecast_date.
    """
    LOOKBACK = int(ctx.lookback)

    forecast_date = pd.Timestamp(forecast_date).normalize()
    base_date = forecast_date

    # History window cho seq model: [base_date - LOOKBACK, base_date)
    start_dt = base_date - pd.Timedelta(hours=LOOKBACK)
    end_dt = base_date

    df_hist = load_slice(
        city=city,
        zone=None if zone == "(All)" else zone,
        routes=[route_id],
        start_dt=start_dt,
        end_dt=end_dt,
    )

    if df_hist is None or df_hist.empty:
        return pd.DataFrame(), seq_model_type

    # Kh·ªüi t·∫°o m·∫∑c ƒë·ªãnh ƒë·ªÉ tr√°nh UnboundLocalError
    df_fc = None
    model_used = seq_model_type

    # ---- RNN ----
    if seq_model_type == "RNN" and getattr(ctx, "rnn_model", None) is not None:
        df_fc, model_used = forecast_rnn(
            route_id=route_id,
            base_date=base_date,
            model=ctx.rnn_model,
            meta=ctx.meta,
            scaler=ctx.scaler,
            routes_model=ctx.routes_model,
            rid2idx=ctx.rid2idx,
            df_hist=df_hist,
        )

    elif seq_model_type == "LSTM":
        # LSTM d√πng artifacts theo family_name c·ªßa ctx (I94, Seattle_FremontBridge, ...)
        from modules.model_utils import forecast_lstm  # n·∫øu b·∫°n ƒë·ªÉ trong module ri√™ng
        lstm_ctx = load_lstm_artifacts_for_family(ctx.family_name)

        if lstm_ctx is not None:
            df_fc, model_used = forecast_lstm(
                route_id=route_id,
                base_date=base_date,
                model=lstm_ctx["model"],
                meta=lstm_ctx["meta"],
                scaler=lstm_ctx["scaler"],
                routes_model=lstm_ctx["routes"],
                rid2idx=lstm_ctx["rid2idx"],
                df_hist=df_hist,
            )
        else:
            # Kh√¥ng c√≥ LSTM ‚Üí tr·∫£ v·ªÅ r·ªóng, ph√≠a tr√™n s·∫Ω b·ªè qua
            df_fc, model_used = pd.DataFrame(), "LSTM_missing"

    else:
        # GRU default
        df_fc, model_used = forecast_gru(
            route_id=route_id,
            base_date=base_date,
            model=ctx.gru_model,
            meta=ctx.meta,
            scaler=ctx.scaler,
            routes_model=ctx.routes_model,
            rid2idx=ctx.rid2idx,
            df_hist=df_hist,
        )

    if df_fc is None or df_fc.empty:
        return pd.DataFrame(), model_used

    df_fc = df_fc.copy()
    df_fc["DateTime"] = pd.to_datetime(df_fc["DateTime"], errors="coerce")
    next_day = forecast_date + pd.Timedelta(days=1)

    df_fc = df_fc[
        (df_fc["DateTime"] >= forecast_date) & (df_fc["DateTime"] < next_day)
    ].sort_values("DateTime")

    return df_fc, model_used

def forecast_week_after_last_point_lstm(
    route_id: str,
    city: str,
    zone: str,
    ctx,
    n_days: int = 7,
):
    """
    Forecast n_days (m·∫∑c ƒë·ªãnh 7) sau NG√ÄY CU·ªêI C√ôNG trong d·ªØ li·ªáu th·∫≠t
    b·∫±ng LSTM, ki·ªÉu NO SHIFT (gi·ªëng forecast_week_after_last_point).
    Tr·∫£ v·ªÅ:
        - df_fc_raw: DataFrame forecast tr√™n timeline th·∫≠t
        - anchor_day_raw: ng√†y cu·ªëi trong d·ªØ li·ªáu (normalize 00:00)
    """
    # 1) Load LSTM artifacts theo family_name (I94, Seattle_FremontBridge, ...)
    lstm_art = load_lstm_artifacts_for_family(ctx.family_name)
    if lstm_art is None:
        print(f"[LSTM-week] Kh√¥ng t√¨m th·∫•y artifacts cho family={ctx.family_name}")
        return pd.DataFrame(), None

    model_lstm = lstm_art["model"]
    meta_lstm = lstm_art["meta"]
    scaler_lstm = lstm_art["scaler"]
    routes_lstm = lstm_art["routes"]
    rid2idx_lstm = lstm_art["rid2idx"]

    # 2) Load to√†n b·ªô series c·ªßa route
    df_full = load_slice(
        city=city,
        zone=zone,
        routes=[route_id],
        start_dt=None,
        end_dt=None,
    )
    if df_full is None or df_full.empty:
        print(f"[LSTM-week] Kh√¥ng c√≥ d·ªØ li·ªáu full cho route={route_id}")
        return pd.DataFrame(), None

    df_full["DateTime"] = pd.to_datetime(df_full["DateTime"], errors="coerce")
    df_full = df_full.dropna(subset=["DateTime", "Vehicles"])
    df_full = df_full.sort_values("DateTime")

    last_dt = df_full["DateTime"].max()
    anchor_day_raw = last_dt.normalize()  # v√≠ d·ª• 2018-10-31 00:00

    # 3) History t·ªïng h·ª£p (ban ƒë·∫ßu = d·ªØ li·ªáu th·∫≠t)
    hist = df_full.copy()

    all_fc = []
    LOOKBACK = ctx.lookback

    for k in range(1, n_days + 1):
        # base_date = ƒë·∫ßu ng√†y th·ª© k sau anchor_day_raw
        base_date = anchor_day_raw + pd.Timedelta(days=k)

        # history LOOKBACK gi·ªù tr∆∞·ªõc base_date
        hist_start = base_date - pd.Timedelta(hours=LOOKBACK)
        df_hist = hist[
            (hist["DateTime"] >= hist_start) & (hist["DateTime"] < base_date)
        ].copy()

        if len(df_hist) < LOOKBACK:
            print(
                f"[LSTM-week] Route {route_id}: thi·∫øu history ({len(df_hist)}h) cho ng√†y {base_date}, d·ª´ng."
            )
            break

        # Forecast 1 ng√†y b·∫±ng LSTM
        df_fc_day, model_used = forecast_lstm(
            route_id=route_id,
            base_date=base_date,
            model=model_lstm,
            meta=meta_lstm,
            scaler=scaler_lstm,
            routes_model=routes_lstm,
            rid2idx=rid2idx_lstm,
            df_hist=df_hist,
        )

        if df_fc_day is None or df_fc_day.empty:
            print(f"[LSTM-week] Forecast r·ªóng cho ng√†y {base_date}, d·ª´ng.")
            break

        all_fc.append(df_fc_day)

        # append prediction v√†o hist ƒë·ªÉ ng√†y sau d√πng lu√¥n c·∫£ data forecast
        tmp = df_fc_day.rename(columns={"PredictedVehicles": "Vehicles"})
        hist = pd.concat(
            [hist, tmp[["DateTime", "Vehicles", "RouteId"]]],
            ignore_index=True,
        )

    if not all_fc:
        return pd.DataFrame(), anchor_day_raw

    df_fc_raw = pd.concat(all_fc, ignore_index=True)
    return df_fc_raw, anchor_day_raw



def vn_weekday_label(dt: pd.Timestamp) -> str:
    """
    Tr·∫£ v·ªÅ label ti·∫øng Vi·ªát cho 1 ng√†y, v√≠ d·ª•: 'Th·ª© 2 21/11'
    """
    dt = pd.Timestamp(dt)
    wd = dt.weekday()  # 0=Mon ... 6=Sun
    if wd == 6:
        thu = "Ch·ªß nh·∫≠t"
    else:
        thu = f"Th·ª© {wd + 2}"
    return f"{thu} {dt.strftime('%d/%m')}"


def load_top2_summary(family_name: str, route_id: str):
    """
    ƒê·ªçc file <route_id>_top2_last_quarter.json n·∫øu c√≥.
    Tr·∫£ v·ªÅ dict ho·∫∑c None.
    """
    model_dir = Path("model") / family_name
    summary_path = model_dir / f"{route_id}_top2_last_quarter.json"
    if not summary_path.exists():
        return None
    try:
        with open(summary_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as ex:
        print(f"[load_top2_summary] Error reading {summary_path}: {ex}")
        return None

# ==== HCMC CONGESTION ‚Äì GRU d·ª± b√°o M·ª©c ƒë·ªô k·∫πt xe 2h t·ªõi ====

HCMC_CSV_PATH = Path("data/raw/hcmc/train.csv")
HCMC_LOOKBACK = 16          # ph·∫£i kh·ªõp v·ªõi LOOKBACK khi train GRU HCMC
HCMC_STEP_MINUTES = 30      # m·ªói period = 30'
HCMC_FC_STEPS = 4           # 4 b∆∞·ªõc = 2 gi·ªù t·ªõi


def render_hcmc_eval_summary_for_route(route_id: str):
    """
    ƒê·ªçc hcmc_eval_summary.csv v√† hi·ªÉn th·ªã MSE / RMSE / MAE / SMAPE / Accuracy
    cho tuy·∫øn HCMC ƒëang ch·ªçn.
    """
    eval_path = os.path.join(BASE_DIR, "data", "hcmc_eval", "hcmc_eval_summary.csv")
    if not os.path.exists(eval_path):
        st.info("Ch∆∞a t√¨m th·∫•y file ƒë√°nh gi√° HCMC (hcmc_eval_summary.csv).")
        return

    df = pd.read_csv(eval_path)

    if "slug" not in df.columns:
        st.warning("File summary kh√¥ng c√≥ c·ªôt 'slug'.")
        return

    row = df[df["slug"] == route_id]
    if row.empty:
        st.info("Ch∆∞a c√≥ metric ƒë√°nh gi√° cho tuy·∫øn n√†y.")
        return

    r = row.iloc[0]

@lru_cache(maxsize=None)
def _load_hcmc_raw_df():
    """ƒê·ªçc raw HCMC + t√≠nh c·ªôt DateTime t·ª´ date + period_x_y."""
    if not HCMC_CSV_PATH.exists():
        print(f"[HCMC] Kh√¥ng t√¨m th·∫•y file {HCMC_CSV_PATH}")
        return None

    df = pd.read_csv(HCMC_CSV_PATH)

    # C·∫ßn t·ªëi thi·ªÉu c√°c c·ªôt n√†y
    if not {"date", "period", "street_name", "LOS"} <= set(df.columns):
        print("[HCMC] Thi·∫øu c·ªôt b·∫Øt bu·ªôc trong train.csv")
        return None

    df["date"] = pd.to_datetime(df["date"])
    period_num = df["period"].str.extract(r"period_(\d+)_(\d+)", expand=True).astype(int)
    df["hour"] = period_num[0]
    df["minute"] = period_num[1]
    df["DateTime"] = (
        df["date"]
        + pd.to_timedelta(df["hour"], unit="h")
        + pd.to_timedelta(df["minute"], unit="m")
    )
    return df


def _load_hcmc_series_for_route(route_id: str, routes_geo_all: pd.DataFrame):
    """
    T·ª´ route_id (slug trong routes_geo) ‚Üí t√¨m street_name g·ªëc trong train.csv,
    r·ªìi build series nh·ªã ph√¢n: 1 = t·∫Øc, 0 = kh√¥ng t·∫Øc. Index = DateTime.
    """
    df_geo = routes_geo_all[
        (routes_geo_all["city"] == "HoChiMinh")
        & (routes_geo_all["route_id"] == route_id)
    ]
    if df_geo.empty:
        print(f"[HCMC] Kh√¥ng t√¨m th·∫•y routes_geo cho route_id={route_id}")
        return None

    full_name = df_geo.iloc[0]["name"]             # VD: "L√Ω Th∆∞·ªùng Ki·ªát (HCMC)"
    street_name = str(full_name).replace(" (HCMC)", "")  # "L√Ω Th∆∞·ªùng Ki·ªát"

    df = _load_hcmc_raw_df()
    if df is None:
        return None

    df_st = df[df["street_name"] == street_name].copy()
    if df_st.empty:
        print(f"[HCMC] Kh√¥ng c√≥ d·ªØ li·ªáu cho street_name='{street_name}'")
        return None

    def is_congested(group: pd.Series) -> int:
        ratio_congested = (group.isin({"D", "E", "F"})).mean()
        return int(ratio_congested >= 0.5)

    s = (
        df_st.groupby("DateTime")["LOS"]
        .apply(is_congested)
        .sort_index()
        .astype(float)
    )
    print(f"[HCMC] '{street_name}': {len(s)} m·ªëc th·ªùi gian (sau group)")
    return s, full_name, street_name

def estimate_travel_time_from_prob(
    p_cong: float,
    length_km: float,
    v_free_kmh: float = HCMC_FREE_FLOW_SPEED_KMH,
) -> tuple[float, float, str]:
    """
    T·ª´ x√°c su·∫•t t·∫Øc ƒë∆∞·ªùng p_cong (0‚Äì1), ∆∞·ªõc l∆∞·ª£ng:
    - th·ªùi gian di chuy·ªÉn ƒë·ªÉ ƒëi h·∫øt tuy·∫øn (ph√∫t)
    - ƒë·ªô tr·ªÖ so v·ªõi ƒëi·ªÅu ki·ªán tho√°ng (ph√∫t)
    - nh√£n m·ª©c ƒë·ªô gi·∫£m t·ªëc (low / medium / high)
    """
    p = float(max(0.0, min(1.0, p_cong)))

    # Th·ªùi gian ƒëi n·∫øu ƒë∆∞·ªùng tho√°ng
    T_free = 60.0 * length_km / max(v_free_kmh, 1e-6)

    # Map p -> h·ªá s·ªë gi·∫£m t·ªëc (speed factor)
    # p th·∫•p => g·∫ßn free-flow; p cao => ch·∫°y ch·∫≠m
    if p <= 0.3:
        factor = 0.9   # g·∫ßn nh∆∞ tho√°ng
        level = "low"
    elif p <= 0.7:
        factor = 0.6   # h∆°i ƒë√¥ng
        level = "medium"
    else:
        factor = 0.3   # r·∫•t ƒë√¥ng
        level = "high"

    v_eff = max(v_free_kmh * factor, 5.0)  # tr√°nh chia cho t·ªëc ƒë·ªô qu√° nh·ªè
    T_travel = 60.0 * length_km / v_eff
    delay = T_travel - T_free
    return T_travel, delay, level


def make_travel_time_table_for_slots(df_slots: "pd.DataFrame", route_id: str) -> "pd.DataFrame":
    """
    Nh·∫≠n v√†o DataFrame c√°c slot d·ª± b√°o 2 gi·ªù t·ªõi v√† route_id,
    tr·∫£ v·ªÅ DataFrame m·ªõi v·ªõi c·ªôt th·ªùi gian di chuy·ªÉn & ƒë·ªô tr·ªÖ.

    ‚ö† Gi·∫£ s·ª≠ df_slots c√≥:
        - c·ªôt 'SlotLabel' (ho·∫∑c 'TimeLabel'): label khung gi·ªù (vd '16:30', '17:00')
        - c·ªôt 'P_cong' (0‚Äì1): x√°c su·∫•t t·∫Øc ƒë∆∞·ªùng trong khung ƒë√≥

    N·∫øu code hi·ªán t·∫°i c·ªßa b·∫°n d√πng t√™n kh√°c, ch·ªâ c·∫ßn ƒë·ªïi l·∫°i cho ƒë√∫ng b√™n d∆∞·ªõi.
    """
    import pandas as pd

    length_km = get_hcmc_route_length_km(route_id)
    v_free = HCMC_FREE_FLOW_SPEED_KMH
    T_free = 60.0 * length_km / max(v_free, 1e-6)

    rows = []
    for _, r in df_slots.iterrows():
        # üëâ ƒê·ªîI t√™n c·ªôt ·ªü ƒë√¢y n·∫øu c·∫ßn:
        p_cong = float(r["P_cong"])  # v√≠ d·ª• n·∫øu c·ªôt l√† 'P_tac' th√¨ s·ª≠a th√†nh r["P_tac"]
        slot_label = str(r["SlotLabel"])  # ho·∫∑c 'TimeLabel', t√πy DataFrame hi·ªán t·∫°i

        T_travel, delay, level = estimate_travel_time_from_prob(p_cong, length_km, v_free)

        rows.append(
            {
                "Khung gi·ªù": slot_label,
                "P t·∫Øc (%)": round(p_cong * 100.0, 1),
                "Th·ªùi gian di chuy·ªÉn (ph√∫t)": round(T_travel, 1),
                "ƒê·ªô tr·ªÖ so v·ªõi ƒë∆∞·ªùng tho√°ng (ph√∫t)": round(delay, 1),
                "M·ª©c ƒë·ªô k·∫πt (low/medium/high)": level,
            }
        )

    df_out = pd.DataFrame(rows)
    # S·∫Øp x·∫øp theo th·ªùi gian n·∫øu c·∫ßn (gi·∫£ s·ª≠ SlotLabel ·ªü d·∫°ng 'HH:MM')
    try:
        df_out = df_out.sort_values("Khung gi·ªù")
    except Exception:
        pass

    # Th√™m T_free v√†o thu·ªôc t√≠nh ƒë·ªÉ hi·ªÉn th·ªã metric nhanh (d√πng getattr b√™n ngo√†i)
    df_out._T_free = T_free
    df_out._length_km = length_km
    return df_out

@st.cache_resource
def _load_hcmc_gru_model_for_route(route_id: str):
    """
    Load model GRU congestion cho 1 tuy·∫øn HCMC.
    Gi·∫£ ƒë·ªãnh file: model/hcmc/gru_congestion_<route_id>.keras
    """
    from tensorflow.keras.models import load_model
    model_path = Path("model") / "hcmc" / f"gru_congestion_{route_id}.keras"
    if not model_path.exists():
        raise FileNotFoundError(f"[HCMC] Kh√¥ng t√¨m th·∫•y model: {model_path}")
    print(f"[HCMC] Load model {model_path}")
    model = load_model(model_path)
    return model


@st.cache_resource
def _load_hcmc_lstm_model_for_route(route_id: str):
    """
    Load model LSTM congestion cho 1 tuy·∫øn HCMC.
    Gi·∫£ ƒë·ªãnh file: model/hcmc/lstm_congestion_<route_id>.keras
    """
    from tensorflow.keras.models import load_model

    model_path = Path("model") / "hcmc" / f"lstm_congestion_{route_id}.keras"
    if not model_path.exists():
        raise FileNotFoundError(f"[HCMC] Kh√¥ng t√¨m th·∫•y model LSTM: {model_path}")

    print(f"[HCMC] Load LSTM model {model_path}")
    model = load_model(model_path)
    return model


def forecast_hcmc_next_2h(route_id: str, routes_geo_all: pd.DataFrame):
    """
    D√πng GRU + LSTM congestion ƒë·ªÉ d·ª± b√°o M·ª©c ƒë·ªô k·∫πt xe cho 4 b∆∞·ªõc ti·∫øp theo (2h t·ªõi).
    Tr·∫£ v·ªÅ (df_fc, full_name) ho·∫∑c None.
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        return None
    s, full_name, street_name = out

    if len(s) <= HCMC_LOOKBACK:
        print(
            f"[HCMC] Qu√° √≠t time step ({len(s)}) cho route_id={route_id}, "
            f"LOOKBACK={HCMC_LOOKBACK}"
        )
        return None

    times = list(s.index)
    y_vals = list(s.values.astype(float))

    def rollout_with_model(model):
        preds = []
        t_local = list(times)
        y_local = list(y_vals)

        for _ in range(HCMC_FC_STEPS):
            window_times = pd.DatetimeIndex(t_local[-HCMC_LOOKBACK:])
            window_y = np.array(y_local[-HCMC_LOOKBACK:], dtype=float)

            total_minutes = window_times.hour * 60 + window_times.minute
            sin_t = np.sin(2 * np.pi * total_minutes / (24 * 60))
            cos_t = np.cos(2 * np.pi * total_minutes / (24 * 60))

            weekday = window_times.weekday
            sin_w = np.sin(2 * np.pi * weekday / 7.0)
            cos_w = np.cos(2 * np.pi * weekday / 7.0)

            F_window = np.stack([window_y, sin_t, cos_t, sin_w, cos_w], axis=1)
            X = F_window[np.newaxis, :, :]

            p = float(model.predict(X, verbose=0).ravel()[0])

            # c·∫≠p nh·∫≠t history b√™n trong "th·∫ø gi·ªõi data"
            last_time = t_local[-1]
            new_time = last_time + pd.Timedelta(minutes=HCMC_STEP_MINUTES)
            t_local.append(new_time)
            y_local.append(1.0 if p >= 0.5 else 0.0)

            preds.append(p)

        return preds

    preds_dict: dict[str, list[float]] = {}

    for model_name, loader in (
        ("GRU", _load_hcmc_gru_model_for_route),
        ("LSTM", _load_hcmc_lstm_model_for_route),
    ):
        try:
            model = loader(route_id)
            preds_dict[model_name] = rollout_with_model(model)
        except FileNotFoundError as ex:
            print(ex)

    if not preds_dict:
        return None

    seq_len = max(len(v) for v in preds_dict.values())

    prob_columns = {}
    for name in ("GRU", "LSTM"):
        vals = preds_dict.get(name)
        if vals is None:
            prob_columns[name] = [np.nan] * seq_len
        elif len(vals) == seq_len:
            prob_columns[name] = vals
        else:
            # b·∫£o ƒë·∫£m c√πng ƒë·ªô d√†i b·∫±ng c√°ch padding NaN ph√≠a sau
            pad_len = seq_len - len(vals)
            prob_columns[name] = vals + [np.nan] * pad_len

    preds_stack = np.array(list(prob_columns.values()), dtype=float)
    preds_avg = np.nanmean(preds_stack, axis=0)

    # --- Ph·∫ßn n√†y l√† M·ªöI: build tr·ª•c th·ªùi gian theo "b√¢y gi·ªù" ---
    now = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh")

    # l√†m tr√≤n v·ªÅ slot g·∫ßn nh·∫•t: 00 ho·∫∑c 30 ph√∫t
    minute_bin = 0 if now.minute < 30 else 30
    current_slot = now.replace(minute=minute_bin, second=0, microsecond=0)

    display_times = [
        current_slot + pd.Timedelta(minutes=HCMC_STEP_MINUTES * (i + 1))
        for i in range(len(preds_avg))
    ]

    df_fc = pd.DataFrame({"DateTime": display_times, "ProbCongested": preds_avg})
    df_fc["Prob_GRU"] = prob_columns["GRU"]
    df_fc["Prob_LSTM"] = prob_columns["LSTM"]

    for name, vals in prob_columns.items():
        df_fc[f"Prob_{name}"] = vals

    return df_fc, full_name


def render_hcmc_congestion_next_2h(route_id: str, routes_geo_all: pd.DataFrame):
    """
    UI cho HCMC: bi·ªÉu ƒë·ªì + b·∫£ng ngang M·ª©c ƒë·ªô k·∫πt xe 2h t·ªõi cho tuy·∫øn ƒëang ch·ªçn,
    + ∆∞·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn theo t·ª´ng khung 30 ph√∫t.
    """
    out = forecast_hcmc_next_2h(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o t·∫Øc ƒë∆∞·ªùng cho tuy·∫øn HCMC n√†y.")
        return

    df_fc, full_name = out

    st.subheader(f"üö¶ D·ª± b√°o nguy c∆° t·∫Øc ƒë∆∞·ªùng trong 2 gi·ªù t·ªõi ‚Äì {full_name}")

    df_fc = df_fc.copy()
    df_fc["DateTime"] = pd.to_datetime(df_fc["DateTime"], errors="coerce")
    df_fc = df_fc.dropna(subset=["DateTime"])
    df_fc["TimeLabel"] = df_fc["DateTime"].dt.strftime("%H:%M")

    def level_from_p(p: float) -> str:
        if p >= 0.7:
            return "high"
        elif p >= 0.4:
            return "medium"
        return "low"

    df_fc["Level"] = df_fc["ProbCongested"].apply(level_from_p)

    # ======== T√ìM T·∫ÆT NHANH 2 GI·ªú T·ªöI ========
    probs = df_fc["ProbCongested"].clip(0.0, 1.0).values
    expected_congested_minutes = HCMC_STEP_MINUTES * float(np.sum(probs))
    avg_prob = float(np.mean(probs))

    avoid_slots = df_fc[df_fc["ProbCongested"] >= 0.7]["TimeLabel"].tolist()
    good_slots = df_fc[df_fc["ProbCongested"] <= 0.3]["TimeLabel"].tolist()

    st.markdown("### T√≥m t·∫Øt nhanh 2 gi·ªù t·ªõi")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Th·ªùi gian k·ª≥ v·ªçng c√≥ nguy c∆° t·∫Øc",
            f"{expected_congested_minutes:,.0f} ph√∫t",
            help="T·ªïng M·ª©c ƒë·ªô k·∫πt xe c·ªßa 4 khung √ó 30 ph√∫t",
        )
    with col2:
        st.metric(
            "S·ªë khung 30' nguy c∆° cao",
            f"{len(avoid_slots)} / {len(df_fc)}",
            help="M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7 ƒë∆∞·ª£c coi l√† nguy c∆° cao",
        )
    with col3:
        st.metric(
            "M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh (GRU/LSTM)",
            f"{avg_prob*100:,.1f} %",
        )

    summary_lines = []
    if avoid_slots:
        summary_lines.append(
            "‚Ä¢ **Khung n√™n tr√°nh** (M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7): " + ", ".join(avoid_slots)
        )
    else:
        summary_lines.append(
            "‚Ä¢ Kh√¥ng c√≥ khung gi·ªù n√†o M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7 trong 2 gi·ªù t·ªõi."
        )

    if good_slots:
        summary_lines.append(
            "‚Ä¢ **Khung n√™n ƒëi** (M·ª©c ƒë·ªô k·∫πt xe ‚â§ 0.3): " + ", ".join(good_slots)
        )
    else:
        summary_lines.append(
            "‚Ä¢ Kh√¥ng c√≥ khung gi·ªù n√†o th·ª±c s·ª± r·∫•t tho√°ng (M·ª©c ƒë·ªô k·∫πt xe ‚â§ 0.3) trong 2 gi·ªù t·ªõi."
        )

    st.markdown("<br>".join(summary_lines), unsafe_allow_html=True)

    # ======== BI·ªÇU ƒê·ªí P(T·∫ÆC) 2H T·ªöI ========
    p_min = float(df_fc["ProbCongested"].min())
    p_max = float(df_fc["ProbCongested"].max())
    span = max(1e-3, p_max - p_min)
    pad = max(0.02, span * 0.3)

    y_low = max(0.0, p_min - pad)
    y_high = min(1.0, p_max + pad)

    base = alt.Chart(df_fc).encode(
        x=alt.X("DateTime:T", title="Th·ªùi gian (30' ti·∫øp theo)"),
    )

    tooltip = [
        alt.Tooltip("DateTime:T", title="Th·ªùi gian"),
        alt.Tooltip("ProbCongested:Q", title="Trung b√¨nh (GRU/LSTM)", format=".2f"),
        alt.Tooltip("Prob_GRU:Q", title="GRU", format=".2f"),
        alt.Tooltip("Prob_LSTM:Q", title="LSTM", format=".2f"),
        alt.Tooltip("Level:N", title="M·ª©c ƒë·ªô"),
    ]

    color_scale = alt.Scale(
        domain=["low", "medium", "high"],
        range=["seagreen", "orange", "red"],
    )

    area = base.mark_area(opacity=0.25).encode(
        y=alt.Y(
            "ProbCongested:Q",
            title="M·ª©c ƒë·ªô k·∫πt xe",
            scale=alt.Scale(domain=[y_low, y_high]),
        ),
        color=alt.value("#eeeeee"),
    )

    line = base.mark_line().encode(
        y=alt.Y(
            "ProbCongested:Q",
            title="M·ª©c ƒë·ªô k·∫πt xe",
            scale=alt.Scale(domain=[y_low, y_high]),
        ),
        tooltip=tooltip,
    )

    points = base.mark_point(size=80).encode(
        y="ProbCongested:Q",
        color=alt.Color(
            "Level:N",
            title="M·ª©c ƒë·ªô t·∫Øc",
            scale=color_scale,
            legend=alt.Legend(
                title="M·ª©c ƒë·ªô t·∫Øc",
                orient="top",
            ),
        ),
        tooltip=tooltip,
    )

    chart = (area + line + points).properties(
        height=260,
        title="D·ª± b√°o x√°c su·∫•t t·∫Øc trong 2 gi·ªù t·ªõi",
    ).interactive()

    st.altair_chart(chart, use_container_width=True)

    # =========================
    # ‚è± ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn trong 2 gi·ªù t·ªõi
    # =========================

    # Chu·∫©n h√≥a df_slots cho h√†m make_travel_time_table_for_slots
    df_slots = df_fc[["TimeLabel", "ProbCongested"]].copy()
    df_slots.rename(
        columns={
            "TimeLabel": "SlotLabel",
            "ProbCongested": "P_cong",
        },
        inplace=True,
    )

    try:
        df_tt = make_travel_time_table_for_slots(df_slots, route_id)
    except Exception as ex:
        st.warning(
            "Kh√¥ng t√≠nh ƒë∆∞·ª£c th·ªùi gian di chuy·ªÉn "
            "(ki·ªÉm tra l·∫°i make_travel_time_table_for_slots / t√™n c·ªôt). "
            f"Chi ti·∫øt: {ex}"
        )
        # v·∫´n ti·∫øp t·ª•c hi·ªÉn th·ªã b·∫£ng ngang m·ª©c ƒë·ªô k·∫πt xe
        df_tt = None

    if df_tt is not None:
        T_free = getattr(df_tt, "_T_free", None)
        length_km = getattr(df_tt, "_length_km", None)

        st.markdown("### ‚è± ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn trong 2 gi·ªù t·ªõi")

        avg_travel = float(df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].mean())
        worst_travel = float(df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].max())
        worst_slot = df_tt.loc[
            df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].idxmax(), "Khung gi·ªù"
        ]

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "Th·ªùi gian trong ƒëi·ªÅu ki·ªán tho√°ng",
                f"{T_free:,.1f} ph√∫t" if T_free is not None else "-",
                help=(
                    f"∆Ø·ªõc t√≠nh v·ªõi chi·ªÅu d√†i tuy·∫øn ~{length_km:.1f} km, "
                    f"t·ªëc ƒë·ªô tho√°ng ~{HCMC_FREE_FLOW_SPEED_KMH:.0f} km/h."
                    if (T_free is not None and length_km is not None)
                    else None
                ),
            )
        with col2:
            st.metric(
                "Th·ªùi gian di chuy·ªÉn trung b√¨nh (4 khung)",
                f"{avg_travel:,.1f} ph√∫t",
            )
        with col3:
            st.metric(
                "T·ªá nh·∫•t trong 2 gi·ªù t·ªõi",
                f"{worst_travel:,.1f} ph√∫t",
                help=f"Khung gi·ªù d·ª± ki·∫øn t·ªën th·ªùi gian nh·∫•t: {worst_slot}.",
            )
        # ==== B·∫£ng ngang M·ª©c ƒë·ªô k·∫πt xe theo t·ª´ng khung 30' ====
        prob_pct = (df_fc.set_index("TimeLabel")["ProbCongested"] * 100).round(1)
        tbl = prob_pct.to_frame().T
        tbl.index = ["M·ª©c ƒë·ªô k·∫πt xe (%)"]

        styled_tbl = (
            tbl.style
            .format("{:,.1f}", na_rep="-")
            .background_gradient(axis=1, cmap="RdYlGn_r")
            .highlight_max(axis=1, color="#8B0000")
        )

        st.dataframe(styled_tbl, use_container_width=True, height=80)

        st.markdown(
            """
            <div style="font-size:0.9rem; margin-top:4px;">
              <b>Ch√∫ th√≠ch m√†u:</b>
              <span style="display:inline-block;width:14px;height:14px;background-color:#006400;border-radius:3px;margin:0 4px 0 8px;border:1px solid #ccc;"></span>
              Xanh = nguy c∆° t·∫Øc th·∫•p
              <span style="display:inline-block;width:14px;height:14px;background-color:#FFD700;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
              V√†ng = trung b√¨nh
              <span style="display:inline-block;width:14px;height:14px;background-color:#8B0000;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
              ƒê·ªè = nguy c∆° t·∫Øc cao
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.markdown("#### B·∫£ng chi ti·∫øt theo t·ª´ng khung 30 ph√∫t")
        st.dataframe(df_tt, use_container_width=True)




def render_hcmc_departure_advisor(route_id: str, routes_geo_all: pd.DataFrame):
    """
    Tr·ª£ l√Ω ch·ªçn gi·ªù ƒëi ƒë∆∞·ªùng cho HCMC:
    - D·ª±a tr√™n l·ªãch s·ª≠ train.csv
    - G·ª£i √Ω khung gi·ªù n√™n ƒëi / n√™n tr√°nh trong ng√†y h√¥m nay
      cho tuy·∫øn ƒë√£ ch·ªçn.
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ t∆∞ v·∫•n gi·ªù ƒëi cho tuy·∫øn n√†y.")
        return

    s, full_name, street_name = out

    # Chu·∫©n b·ªã DataFrame l·ªãch s·ª≠: m·ªói m·ªëc th·ªùi gian = 0/1 (k·∫πt / kh√¥ng)
    df_hist = s.to_frame(name="is_congested")
    df_hist["DateTime"] = df_hist.index
    df_hist["hour"] = df_hist["DateTime"].dt.hour
    df_hist["minute"] = df_hist["DateTime"].dt.minute
    df_hist["weekday"] = df_hist["DateTime"].dt.weekday

    st.subheader("üß≠ Tr·ª£ l√Ω ch·ªçn gi·ªù ƒëi ƒë∆∞·ªùng")

    st.markdown(
        f"D·ª±a tr√™n d·ªØ li·ªáu l·ªãch s·ª≠ c·ªßa tuy·∫øn **{full_name}**, "
        "g·ª£i √Ω khung gi·ªù n√™n ƒëi / n√™n tr√°nh cho **ng√†y h√¥m nay**."
    )

    now = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh")
    today_wd = now.weekday()

    # Ch·ªçn khung gi·ªù quan t√¢m
    window_label = st.selectbox(
        "Ch·ªçn khung gi·ªù b·∫°n quan t√¢m",
        ["S√°ng (06:00‚Äì09:00)", "Chi·ªÅu (16:00‚Äì19:00)"],
        key="hcmc_advisor_window",
    )

    if window_label.startswith("S√°ng"):
        start_hour, end_hour = 6, 9
    else:
        start_hour, end_hour = 16, 19

    # T·∫°o list slot 30' trong kho·∫£ng [start_hour, end_hour)
    slots = []
    h = start_hour
    m = 0
    while h < end_hour:
        slots.append((h, m))
        if m == 0:
            m = 30
        else:
            m = 0
            h += 1

    rows = []
    for (h, m) in slots:
        subset = df_hist[(df_hist["hour"] == h) & (df_hist["minute"] == m)]
        if subset.empty:
            mean_cong = np.nan
        else:
            # ∆Øu ti√™n d√πng ƒë√∫ng th·ª© trong tu·∫ßn h√¥m nay, n·∫øu ƒë·ªß m·∫´u
            subset_today = subset[subset["weekday"] == today_wd]
            if len(subset_today) >= 5:
                mean_cong = subset_today["is_congested"].mean()
            else:
                mean_cong = subset["is_congested"].mean()
        rows.append({"hour": h, "minute": m, "MeanCongestion": mean_cong})

    df_window = pd.DataFrame(rows).dropna(subset=["MeanCongestion"])
    if df_window.empty:
        st.info(
            "Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ t∆∞ v·∫•n khung gi·ªù cho tuy·∫øn n√†y trong kho·∫£ng ƒë√£ ch·ªçn."
        )
        return

    df_window["TimeLabel"] = df_window.apply(
        lambda r: f"{int(r['hour']):02d}:{int(r['minute']):02d}", axis=1
    )
    df_window["CongestionPct"] = (df_window["MeanCongestion"] * 100.0).round(1)

    # ====== T√¨m khung n√™n ƒëi / n√™n tr√°nh theo ng∆∞·ª°ng ph·∫ßn trƒÉm ======
    avg_pct = float(df_window["CongestionPct"].mean())

    GOOD_THR = 30.0  # <= 30%: n√™n ƒëi
    BAD_THR = 70.0   # >= 70%: n√™n tr√°nh

    good = df_window[df_window["CongestionPct"] <= GOOD_THR]
    bad = df_window[df_window["CongestionPct"] >= BAD_THR]

    # Khung n√™n ƒëi: ∆∞u ti√™n t·∫•t c·∫£ khung "good"; n·∫øu kh√¥ng c√≥ th√¨ l·∫•y 1‚Äì2 khung nh·ªè nh·∫•t
    if not good.empty:
        best_list = (
            good.sort_values("CongestionPct")[["TimeLabel"]]
            .drop_duplicates()
            .iloc[:, 0]
            .tolist()
        )
    else:
        best_list = (
            df_window.nsmallest(2, "CongestionPct")[["TimeLabel"]]
            .iloc[:, 0]
            .tolist()
        )

    # Khung n√™n tr√°nh: ∆∞u ti√™n t·∫•t c·∫£ khung "bad"; n·∫øu kh√¥ng c√≥ v√† c√≥ k·∫πt >0% th√¨ l·∫•y 1‚Äì2 khung l·ªõn nh·∫•t
    if not bad.empty:
        worst_list = (
            bad.sort_values("CongestionPct", ascending=False)[["TimeLabel"]]
            .drop_duplicates()
            .iloc[:, 0]
            .tolist()
        )
    else:
        if df_window["CongestionPct"].max() > 0:
            worst_list = (
                df_window.nlargest(2, "CongestionPct")[["TimeLabel"]]
                .iloc[:, 0]
                .tolist()
            )
        else:
            worst_list = []

    best_str = ", ".join(best_list)
    worst_str = ", ".join(worst_list)


    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Khung n√™n ƒëi (√≠t k·∫πt nh·∫•t)",
            best_str or "-",
        )
    with col2:
        st.metric(
            "Khung n√™n tr√°nh (k·∫πt nh·∫•t)",
            worst_str or "-",
        )
    with col3:
        st.metric(
            "M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh",
            f"{avg_pct:,.1f} %",
        )

    # st.markdown(
    #     f"- **Khung n√™n ƒëi**: {best_str if best_str else 'ch∆∞a r√µ do thi·∫øu d·ªØ li·ªáu'}  \n"
    #     f"- **Khung n√™n tr√°nh**: {worst_str if worst_str else 'ch∆∞a r√µ do thi·∫øu d·ªØ li·ªáu'}"
    # )

    # Bi·ªÉu ƒë·ªì c·ªôt m·ª©c ƒë·ªô k·∫πt theo t·ª´ng slot
    chart = (
        alt.Chart(df_window)
        .mark_bar()
        .encode(
            x=alt.X("TimeLabel:N", title="Khung gi·ªù (30 ph√∫t)"),
            y=alt.Y(
                "CongestionPct:Q",
                title="M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh (%)",
            ),
            color=alt.Color(
                "CongestionPct:Q",
                scale=alt.Scale(scheme="RdYlGn_r"),  # th·∫•p = xanh, cao = ƒë·ªè
                legend=alt.Legend(title="K·∫πt xe (%)"),
            ),
            tooltip=[
                alt.Tooltip("TimeLabel:N", title="Khung gi·ªù"),
                alt.Tooltip(
                    "CongestionPct:Q",
                    title="M·ª©c ƒë·ªô k·∫πt xe (%)",
                    format=".1f",
                ),
            ],
        )
        .properties(height=260, title="M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh theo khung 30 ph√∫t")
    )

    st.altair_chart(chart, use_container_width=True)

def render_hcmc_weekly_pattern(route_id: str, routes_geo_all: pd.DataFrame):
    """
    Hi·ªÉn th·ªã 'heatmap' m·∫´u h√¨nh k·∫πt xe theo gi·ªù & th·ª© trong tu·∫ßn
    cho m·ªôt tuy·∫øn HCMC, d·∫°ng b·∫£ng m√†u (pandas.style).
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    s, full_name, street_name = out

    df = s.to_frame(name="is_congested")
    if df.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    df["DateTime"] = df.index
    df["hour"] = df["DateTime"].dt.hour
    df["weekday"] = df["DateTime"].dt.weekday  # 0=Mon ... 6=Sun

    weekday_map = {
        0: "Th·ª© 2",
        1: "Th·ª© 3",
        2: "Th·ª© 4",
        3: "Th·ª© 5",
        4: "Th·ª© 6",
        5: "Th·ª© 7",
        6: "Ch·ªß nh·∫≠t",
    }
    df["weekday_label"] = df["weekday"].map(weekday_map)

    # Nh√≥m theo (weekday_label, hour) ƒë·ªÉ l·∫•y t·ªâ l·ªá k·∫πt trung b√¨nh
    grp = (
        df.groupby(["weekday_label", "hour"], as_index=False)["is_congested"]
        .mean()
    )
    if grp.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    grp["CongestionPct"] = (grp["is_congested"] * 100.0).round(1)
    grp["HourStr"] = grp["hour"].astype(int).astype(str).str.zfill(2) + ":00"

    st.subheader("üìÖ M·∫´u h√¨nh k·∫πt xe trong tu·∫ßn theo gi·ªù")
    st.markdown(
        "M√†u c√†ng ƒë·ªè = tuy·∫øn c√†ng th∆∞·ªùng xuy√™n k·∫πt t·∫°i khung gi·ªù ƒë√≥ "
        "(t√≠nh theo l·ªãch s·ª≠ trong t·∫≠p d·ªØ li·ªáu HCMC)."
    )

    # Pivot th√†nh b·∫£ng 7 x 24 (th·ª© x gi·ªù)
    pivot = grp.pivot_table(
        index="weekday_label",
        columns="HourStr",
        values="CongestionPct",
        aggfunc="mean",
    )

    # S·∫Øp x·∫øp th·ª© theo ƒë√∫ng th·ª© t·ª±
    order_idx = ["Th·ª© 2", "Th·ª© 3", "Th·ª© 4", "Th·ª© 5", "Th·ª© 6", "Th·ª© 7", "Ch·ªß nh·∫≠t"]
    pivot = pivot.reindex(order_idx)

    # S·∫Øp x·∫øp gi·ªù theo th·ª© t·ª± th·ªùi gian
    pivot = pivot.reindex(sorted(pivot.columns), axis=1)

    # ƒê·∫£m b·∫£o gi√° tr·ªã l√† float (NaN chu·∫©n)
    pivot_float = pivot.astype("float")

    # H√†m style ri√™ng cho √¥ kh√¥ng c√≥ d·ªØ li·ªáu
    def style_na(v):
        if pd.isna(v):
            # n·ªÅn tr·∫Øng, ch·ªØ x√°m nh·∫°t (c√≥ th·ªÉ ƒë·ªïi 'No data' t√πy th√≠ch)
            return "background-color: #ffffff; color: #999999;"
        return ""

    styled = (
        pivot_float.style
        # t√¥ heatmap cho c√°c √¥ c√≥ s·ªë
        .background_gradient(cmap="RdYlGn_r", axis=None)
        # format s·ªë, √¥ NaN th√¨ ƒë·ªÉ tr·ªëng ho·∫∑c ghi 'None' t√πy b·∫°n
        .format("{:.1f}", na_rep="None")   # ho·∫∑c na_rep="" n·∫øu mu·ªën √¥ tr·ªëng
        # override l·∫°i style cho √¥ NaN (ƒë·∫∑t sau background_gradient ƒë·ªÉ ƒë√® m√†u)
        .applymap(style_na)
    )

    st.dataframe(styled, use_container_width=True)

# ==== HCMC CONGESTION ‚Äì GRU d·ª± b√°o M·ª©c ƒë·ªô k·∫πt xe 2h t·ªõi ====

HCMC_CSV_PATH = Path("data/raw/hcmc/train.csv")
HCMC_LOOKBACK = 16          # ph·∫£i kh·ªõp v·ªõi LOOKBACK khi train GRU HCMC
HCMC_STEP_MINUTES = 30      # m·ªói period = 30'
HCMC_FC_STEPS = 4           # 4 b∆∞·ªõc = 2 gi·ªù t·ªõi


def render_hcmc_eval_summary_for_route(route_id: str):
    """
    ƒê·ªçc hcmc_eval_summary.csv v√† hi·ªÉn th·ªã MSE / RMSE / MAE / SMAPE / Accuracy
    cho tuy·∫øn HCMC ƒëang ch·ªçn.
    """
    eval_path = os.path.join(BASE_DIR, "data", "hcmc_eval", "hcmc_eval_summary.csv")
    if not os.path.exists(eval_path):
        st.info("Ch∆∞a t√¨m th·∫•y file ƒë√°nh gi√° HCMC (hcmc_eval_summary.csv).")
        return

    df = pd.read_csv(eval_path)

    if "slug" not in df.columns:
        st.warning("File summary kh√¥ng c√≥ c·ªôt 'slug'.")
        return

    row = df[df["slug"] == route_id]
    if row.empty:
        st.info("Ch∆∞a c√≥ metric ƒë√°nh gi√° cho tuy·∫øn n√†y.")
        return

    r = row.iloc[0]

    st.markdown("### üìä ƒê√°nh gi√° ƒë·ªô tin c·∫≠y m√¥ h√¨nh (HCMC)")

    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("MSE", f"{r['MSE']:.4f}")
    with col2:
        st.metric("RMSE", f"{r['RMSE']:.4f}")
    with col3:
        st.metric("MAE", f"{r['MAE']:.4f}")

    # col4, col5 = st.columns(2)
    with col4:
        st.metric("SMAPE", f"{r['SMAPE']:.2f} %")
    with col5:
        st.metric("Accuracy", f"{r['Accuracy']:.1f} %")


@lru_cache(maxsize=None)
def _load_hcmc_raw_df():
    """ƒê·ªçc raw HCMC + t√≠nh c·ªôt DateTime t·ª´ date + period_x_y."""
    if not HCMC_CSV_PATH.exists():
        print(f"[HCMC] Kh√¥ng t√¨m th·∫•y file {HCMC_CSV_PATH}")
        return None

    df = pd.read_csv(HCMC_CSV_PATH)

    # C·∫ßn t·ªëi thi·ªÉu c√°c c·ªôt n√†y
    if not {"date", "period", "street_name", "LOS"} <= set(df.columns):
        print("[HCMC] Thi·∫øu c·ªôt b·∫Øt bu·ªôc trong train.csv")
        return None

    df["date"] = pd.to_datetime(df["date"])
    period_num = df["period"].str.extract(r"period_(\d+)_(\d+)", expand=True).astype(int)
    df["hour"] = period_num[0]
    df["minute"] = period_num[1]
    df["DateTime"] = (
        df["date"]
        + pd.to_timedelta(df["hour"], unit="h")
        + pd.to_timedelta(df["minute"], unit="m")
    )
    return df


def _load_hcmc_series_for_route(route_id: str, routes_geo_all: pd.DataFrame):
    """
    T·ª´ route_id (slug trong routes_geo) ‚Üí t√¨m street_name g·ªëc trong train.csv,
    r·ªìi build series nh·ªã ph√¢n: 1 = t·∫Øc, 0 = kh√¥ng t·∫Øc. Index = DateTime.
    """
    df_geo = routes_geo_all[
        (routes_geo_all["city"] == "HoChiMinh")
        & (routes_geo_all["route_id"] == route_id)
    ]
    if df_geo.empty:
        print(f"[HCMC] Kh√¥ng t√¨m th·∫•y routes_geo cho route_id={route_id}")
        return None

    full_name = df_geo.iloc[0]["name"]             # VD: "L√Ω Th∆∞·ªùng Ki·ªát (HCMC)"
    street_name = str(full_name).replace(" (HCMC)", "")  # "L√Ω Th∆∞·ªùng Ki·ªát"

    df = _load_hcmc_raw_df()
    if df is None:
        return None

    df_st = df[df["street_name"] == street_name].copy()
    if df_st.empty:
        print(f"[HCMC] Kh√¥ng c√≥ d·ªØ li·ªáu cho street_name='{street_name}'")
        return None

    def is_congested(group: pd.Series) -> int:
        ratio_congested = (group.isin({"D", "E", "F"})).mean()
        return int(ratio_congested >= 0.5)

    s = (
        df_st.groupby("DateTime")["LOS"]
        .apply(is_congested)
        .sort_index()
        .astype(float)
    )
    print(f"[HCMC] '{street_name}': {len(s)} m·ªëc th·ªùi gian (sau group)")
    return s, full_name, street_name

def estimate_travel_time_from_prob(
    p_cong: float,
    length_km: float,
    v_free_kmh: float = HCMC_FREE_FLOW_SPEED_KMH,
) -> tuple[float, float, str]:
    """
    T·ª´ x√°c su·∫•t t·∫Øc ƒë∆∞·ªùng p_cong (0‚Äì1), ∆∞·ªõc l∆∞·ª£ng:
    - th·ªùi gian di chuy·ªÉn ƒë·ªÉ ƒëi h·∫øt tuy·∫øn (ph√∫t)
    - ƒë·ªô tr·ªÖ so v·ªõi ƒëi·ªÅu ki·ªán tho√°ng (ph√∫t)
    - nh√£n m·ª©c ƒë·ªô gi·∫£m t·ªëc (low / medium / high)
    """
    p = float(max(0.0, min(1.0, p_cong)))

    # Th·ªùi gian ƒëi n·∫øu ƒë∆∞·ªùng tho√°ng
    T_free = 60.0 * length_km / max(v_free_kmh, 1e-6)

    # Map p -> h·ªá s·ªë gi·∫£m t·ªëc (speed factor)
    # p th·∫•p => g·∫ßn free-flow; p cao => ch·∫°y ch·∫≠m
    if p <= 0.3:
        factor = 0.9   # g·∫ßn nh∆∞ tho√°ng
        level = "low"
    elif p <= 0.7:
        factor = 0.6   # h∆°i ƒë√¥ng
        level = "medium"
    else:
        factor = 0.3   # r·∫•t ƒë√¥ng
        level = "high"

    v_eff = max(v_free_kmh * factor, 5.0)  # tr√°nh chia cho t·ªëc ƒë·ªô qu√° nh·ªè
    T_travel = 60.0 * length_km / v_eff
    delay = T_travel - T_free
    return T_travel, delay, level


def make_travel_time_table_for_slots(df_slots: "pd.DataFrame", route_id: str) -> "pd.DataFrame":
    """
    Nh·∫≠n v√†o DataFrame c√°c slot d·ª± b√°o 2 gi·ªù t·ªõi v√† route_id,
    tr·∫£ v·ªÅ DataFrame m·ªõi v·ªõi c·ªôt th·ªùi gian di chuy·ªÉn & ƒë·ªô tr·ªÖ.

    ‚ö† Gi·∫£ s·ª≠ df_slots c√≥:
        - c·ªôt 'SlotLabel' (ho·∫∑c 'TimeLabel'): label khung gi·ªù (vd '16:30', '17:00')
        - c·ªôt 'P_cong' (0‚Äì1): x√°c su·∫•t t·∫Øc ƒë∆∞·ªùng trong khung ƒë√≥

    N·∫øu code hi·ªán t·∫°i c·ªßa b·∫°n d√πng t√™n kh√°c, ch·ªâ c·∫ßn ƒë·ªïi l·∫°i cho ƒë√∫ng b√™n d∆∞·ªõi.
    """
    import pandas as pd

    length_km = get_hcmc_route_length_km(route_id)
    v_free = HCMC_FREE_FLOW_SPEED_KMH
    T_free = 60.0 * length_km / max(v_free, 1e-6)

    rows = []
    for _, r in df_slots.iterrows():
        # üëâ ƒê·ªîI t√™n c·ªôt ·ªü ƒë√¢y n·∫øu c·∫ßn:
        p_cong = float(r["P_cong"])  # v√≠ d·ª• n·∫øu c·ªôt l√† 'P_tac' th√¨ s·ª≠a th√†nh r["P_tac"]
        slot_label = str(r["SlotLabel"])  # ho·∫∑c 'TimeLabel', t√πy DataFrame hi·ªán t·∫°i

        T_travel, delay, level = estimate_travel_time_from_prob(p_cong, length_km, v_free)

        rows.append(
            {
                "Khung gi·ªù": slot_label,
                "P t·∫Øc (%)": round(p_cong * 100.0, 1),
                "Th·ªùi gian di chuy·ªÉn (ph√∫t)": round(T_travel, 1),
                "ƒê·ªô tr·ªÖ so v·ªõi ƒë∆∞·ªùng tho√°ng (ph√∫t)": round(delay, 1),
                "M·ª©c ƒë·ªô k·∫πt (low/medium/high)": level,
            }
        )

    df_out = pd.DataFrame(rows)
    # S·∫Øp x·∫øp theo th·ªùi gian n·∫øu c·∫ßn (gi·∫£ s·ª≠ SlotLabel ·ªü d·∫°ng 'HH:MM')
    try:
        df_out = df_out.sort_values("Khung gi·ªù")
    except Exception:
        pass

    # Th√™m T_free v√†o thu·ªôc t√≠nh ƒë·ªÉ hi·ªÉn th·ªã metric nhanh (d√πng getattr b√™n ngo√†i)
    df_out._T_free = T_free
    df_out._length_km = length_km
    return df_out

@st.cache_resource
def _load_hcmc_gru_model_for_route(route_id: str):
    """
    Load model GRU congestion cho 1 tuy·∫øn HCMC.
    Gi·∫£ ƒë·ªãnh file: model/hcmc/gru_congestion_<route_id>.keras
    """
    from tensorflow.keras.models import load_model
    model_path = Path("model") / "hcmc" / f"gru_congestion_{route_id}.keras"
    if not model_path.exists():
        raise FileNotFoundError(f"[HCMC] Kh√¥ng t√¨m th·∫•y model: {model_path}")
    print(f"[HCMC] Load model {model_path}")
    model = load_model(model_path)
    return model


def forecast_hcmc_next_2h(route_id: str, routes_geo_all: pd.DataFrame):
    """
    D√πng GRU congestion ƒë·ªÉ d·ª± b√°o M·ª©c ƒë·ªô k·∫πt xe cho 4 b∆∞·ªõc ti·∫øp theo (2h t·ªõi).
    Tr·∫£ v·ªÅ (df_fc, full_name) ho·∫∑c None.
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        return None
    s, full_name, street_name = out

    if len(s) <= HCMC_LOOKBACK:
        print(
            f"[HCMC] Qu√° √≠t time step ({len(s)}) cho route_id={route_id}, "
            f"LOOKBACK={HCMC_LOOKBACK}"
        )
        return None

    times = list(s.index)
    y_vals = list(s.values.astype(float))

    model = _load_hcmc_gru_model_for_route(route_id)

    preds = []

    for _ in range(HCMC_FC_STEPS):
        window_times = pd.DatetimeIndex(times[-HCMC_LOOKBACK:])
        window_y = np.array(y_vals[-HCMC_LOOKBACK:], dtype=float)

        total_minutes = window_times.hour * 60 + window_times.minute
        sin_t = np.sin(2 * np.pi * total_minutes / (24 * 60))
        cos_t = np.cos(2 * np.pi * total_minutes / (24 * 60))

        weekday = window_times.weekday
        sin_w = np.sin(2 * np.pi * weekday / 7.0)
        cos_w = np.cos(2 * np.pi * weekday / 7.0)

        F_window = np.stack([window_y, sin_t, cos_t, sin_w, cos_w], axis=1)
        X = F_window[np.newaxis, :, :]

        p = float(model.predict(X, verbose=0).ravel()[0])

        # c·∫≠p nh·∫≠t history b√™n trong "th·∫ø gi·ªõi data"
        last_time = times[-1]
        new_time = last_time + pd.Timedelta(minutes=HCMC_STEP_MINUTES)
        times.append(new_time)
        y_vals.append(1.0 if p >= 0.5 else 0.0)

        preds.append(p)

    # --- Ph·∫ßn n√†y l√† M·ªöI: build tr·ª•c th·ªùi gian theo "b√¢y gi·ªù" ---
    now = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh")

    # l√†m tr√≤n v·ªÅ slot g·∫ßn nh·∫•t: 00 ho·∫∑c 30 ph√∫t
    minute_bin = 0 if now.minute < 30 else 30
    current_slot = now.replace(minute=minute_bin, second=0, microsecond=0)

    display_times = [
        current_slot + pd.Timedelta(minutes=HCMC_STEP_MINUTES * (i + 1))
        for i in range(len(preds))
    ]

    df_fc = pd.DataFrame(
        {
            "DateTime": display_times,
            "ProbCongested": preds,
        }
    )
    return df_fc, full_name

    return df_fc, full_name


def render_hcmc_congestion_next_2h(route_id: str, routes_geo_all: pd.DataFrame):
    """
    UI cho HCMC: bi·ªÉu ƒë·ªì + b·∫£ng ngang M·ª©c ƒë·ªô k·∫πt xe 2h t·ªõi cho tuy·∫øn ƒëang ch·ªçn,
    + ∆∞·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn theo t·ª´ng khung 30 ph√∫t.
    """
    out = forecast_hcmc_next_2h(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o t·∫Øc ƒë∆∞·ªùng cho tuy·∫øn HCMC n√†y.")
        return

    df_fc, full_name = out

    st.subheader(f"üö¶ D·ª± b√°o nguy c∆° t·∫Øc ƒë∆∞·ªùng trong 2 gi·ªù t·ªõi ‚Äì {full_name}")

    df_fc = df_fc.copy()
    df_fc["DateTime"] = pd.to_datetime(df_fc["DateTime"], errors="coerce")
    df_fc = df_fc.dropna(subset=["DateTime"])
    df_fc["TimeLabel"] = df_fc["DateTime"].dt.strftime("%H:%M")

    def level_from_p(p: float) -> str:
        if p >= 0.7:
            return "high"
        elif p >= 0.4:
            return "medium"
        return "low"

    df_fc["Level"] = df_fc["ProbCongested"].apply(level_from_p)

    # ======== T√ìM T·∫ÆT NHANH 2 GI·ªú T·ªöI ========
    probs = df_fc["ProbCongested"].clip(0.0, 1.0).values
    expected_congested_minutes = HCMC_STEP_MINUTES * float(np.sum(probs))
    avg_prob = float(np.mean(probs))

    avoid_slots = df_fc[df_fc["ProbCongested"] >= 0.7]["TimeLabel"].tolist()
    good_slots = df_fc[df_fc["ProbCongested"] <= 0.3]["TimeLabel"].tolist()

    st.markdown("### T√≥m t·∫Øt nhanh 2 gi·ªù t·ªõi")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Th·ªùi gian k·ª≥ v·ªçng c√≥ nguy c∆° t·∫Øc",
            f"{expected_congested_minutes:,.0f} ph√∫t",
            help="T·ªïng M·ª©c ƒë·ªô k·∫πt xe c·ªßa 4 khung √ó 30 ph√∫t",
        )
    with col2:
        st.metric(
            "S·ªë khung 30' nguy c∆° cao",
            f"{len(avoid_slots)} / {len(df_fc)}",
            help="M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7 ƒë∆∞·ª£c coi l√† nguy c∆° cao",
        )
    with col3:
        st.metric(
            "M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh (2h t·ªõi)",
            f"{avg_prob*100:,.1f} %",
        )

    summary_lines = []
    if avoid_slots:
        summary_lines.append(
            "‚Ä¢ **Khung n√™n tr√°nh** (M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7): " + ", ".join(avoid_slots)
        )
    else:
        summary_lines.append(
            "‚Ä¢ Kh√¥ng c√≥ khung gi·ªù n√†o M·ª©c ƒë·ªô k·∫πt xe ‚â• 0.7 trong 2 gi·ªù t·ªõi."
        )

    if good_slots:
        summary_lines.append(
            "‚Ä¢ **Khung n√™n ƒëi** (M·ª©c ƒë·ªô k·∫πt xe ‚â§ 0.3): " + ", ".join(good_slots)
        )
    else:
        summary_lines.append(
            "‚Ä¢ Kh√¥ng c√≥ khung gi·ªù n√†o th·ª±c s·ª± r·∫•t tho√°ng (M·ª©c ƒë·ªô k·∫πt xe ‚â§ 0.3) trong 2 gi·ªù t·ªõi."
        )

    st.markdown("<br>".join(summary_lines), unsafe_allow_html=True)

    # ======== BI·ªÇU ƒê·ªí P(T·∫ÆC) 2H T·ªöI ========
    p_min = float(df_fc["ProbCongested"].min())
    p_max = float(df_fc["ProbCongested"].max())
    span = max(1e-3, p_max - p_min)
    pad = max(0.02, span * 0.3)

    y_low = max(0.0, p_min - pad)
    y_high = min(1.0, p_max + pad)

    base = alt.Chart(df_fc).encode(
        x=alt.X("DateTime:T", title="Th·ªùi gian (30' ti·∫øp theo)"),
    )

    tooltip = [
        alt.Tooltip("DateTime:T", title="Th·ªùi gian"),
        alt.Tooltip("ProbCongested:Q", title="M·ª©c ƒë·ªô k·∫πt xe", format=".2f"),
        alt.Tooltip("Level:N", title="M·ª©c ƒë·ªô"),
    ]

    color_scale = alt.Scale(
        domain=["low", "medium", "high"],
        range=["seagreen", "orange", "red"],
    )

    area = base.mark_area(opacity=0.25).encode(
        y=alt.Y(
            "ProbCongested:Q",
            title="M·ª©c ƒë·ªô k·∫πt xe",
            scale=alt.Scale(domain=[y_low, y_high]),
        ),
        color=alt.value("#eeeeee"),
    )

    line = base.mark_line().encode(
        y=alt.Y(
            "ProbCongested:Q",
            title="M·ª©c ƒë·ªô k·∫πt xe",
            scale=alt.Scale(domain=[y_low, y_high]),
        ),
        tooltip=tooltip,
    )

    points = base.mark_point(size=80).encode(
        y="ProbCongested:Q",
        color=alt.Color(
            "Level:N",
            title="M·ª©c ƒë·ªô t·∫Øc",
            scale=color_scale,
            legend=alt.Legend(
                title="M·ª©c ƒë·ªô t·∫Øc",
                orient="top",
            ),
        ),
        tooltip=tooltip,
    )

    chart = (area + line + points).properties(
        height=260,
        title="D·ª± b√°o x√°c su·∫•t t·∫Øc trong 2 gi·ªù t·ªõi",
    ).interactive()

    st.altair_chart(chart, use_container_width=True)

    # =========================
    # ‚è± ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn trong 2 gi·ªù t·ªõi
    # =========================

    # Chu·∫©n h√≥a df_slots cho h√†m make_travel_time_table_for_slots
    df_slots = df_fc[["TimeLabel", "ProbCongested"]].copy()
    df_slots.rename(
        columns={
            "TimeLabel": "SlotLabel",
            "ProbCongested": "P_cong",
        },
        inplace=True,
    )

    try:
        df_tt = make_travel_time_table_for_slots(df_slots, route_id)
    except Exception as ex:
        st.warning(
            "Kh√¥ng t√≠nh ƒë∆∞·ª£c th·ªùi gian di chuy·ªÉn "
            "(ki·ªÉm tra l·∫°i make_travel_time_table_for_slots / t√™n c·ªôt). "
            f"Chi ti·∫øt: {ex}"
        )
        # v·∫´n ti·∫øp t·ª•c hi·ªÉn th·ªã b·∫£ng ngang m·ª©c ƒë·ªô k·∫πt xe
        df_tt = None

    if df_tt is not None:
        T_free = getattr(df_tt, "_T_free", None)
        length_km = getattr(df_tt, "_length_km", None)

        st.markdown("### ‚è± ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian di chuy·ªÉn trong 2 gi·ªù t·ªõi")

        avg_travel = float(df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].mean())
        worst_travel = float(df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].max())
        worst_slot = df_tt.loc[
            df_tt["Th·ªùi gian di chuy·ªÉn (ph√∫t)"].idxmax(), "Khung gi·ªù"
        ]

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "Th·ªùi gian trong ƒëi·ªÅu ki·ªán tho√°ng",
                f"{T_free:,.1f} ph√∫t" if T_free is not None else "-",
                help=(
                    f"∆Ø·ªõc t√≠nh v·ªõi chi·ªÅu d√†i tuy·∫øn ~{length_km:.1f} km, "
                    f"t·ªëc ƒë·ªô tho√°ng ~{HCMC_FREE_FLOW_SPEED_KMH:.0f} km/h."
                    if (T_free is not None and length_km is not None)
                    else None
                ),
            )
        with col2:
            st.metric(
                "Th·ªùi gian di chuy·ªÉn trung b√¨nh (4 khung)",
                f"{avg_travel:,.1f} ph√∫t",
            )
        with col3:
            st.metric(
                "T·ªá nh·∫•t trong 2 gi·ªù t·ªõi",
                f"{worst_travel:,.1f} ph√∫t",
                help=f"Khung gi·ªù d·ª± ki·∫øn t·ªën th·ªùi gian nh·∫•t: {worst_slot}.",
            )

        st.markdown("#### B·∫£ng chi ti·∫øt theo t·ª´ng khung 30 ph√∫t")
        st.dataframe(df_tt, use_container_width=True)

    # ==== B·∫£ng ngang M·ª©c ƒë·ªô k·∫πt xe theo t·ª´ng khung 30' ====
    prob_pct = (df_fc.set_index("TimeLabel")["ProbCongested"] * 100).round(1)
    tbl = prob_pct.to_frame().T
    tbl.index = ["M·ª©c ƒë·ªô k·∫πt xe (%)"]

    styled_tbl = (
        tbl.style
        .format("{:,.1f}", na_rep="-")
        .background_gradient(axis=1, cmap="RdYlGn_r")
        .highlight_max(axis=1, color="#8B0000")
    )

    st.dataframe(styled_tbl, use_container_width=True, height=80)

    st.markdown(
        """
        <div style="font-size:0.9rem; margin-top:4px;">
          <b>Ch√∫ th√≠ch m√†u:</b>
          <span style="display:inline-block;width:14px;height:14px;background-color:#006400;border-radius:3px;margin:0 4px 0 8px;border:1px solid #ccc;"></span>
          Xanh = nguy c∆° t·∫Øc th·∫•p
          <span style="display:inline-block;width:14px;height:14px;background-color:#FFD700;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
          V√†ng = trung b√¨nh
          <span style="display:inline-block;width:14px;height:14px;background-color:#8B0000;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
          ƒê·ªè = nguy c∆° t·∫Øc cao
        </div>
        """,
        unsafe_allow_html=True,
    )


def render_hcmc_departure_advisor(route_id: str, routes_geo_all: pd.DataFrame):
    """
    Tr·ª£ l√Ω ch·ªçn gi·ªù ƒëi ƒë∆∞·ªùng cho HCMC:
    - D·ª±a tr√™n l·ªãch s·ª≠ train.csv
    - G·ª£i √Ω khung gi·ªù n√™n ƒëi / n√™n tr√°nh trong ng√†y h√¥m nay
      cho tuy·∫øn ƒë√£ ch·ªçn.
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ t∆∞ v·∫•n gi·ªù ƒëi cho tuy·∫øn n√†y.")
        return

    s, full_name, street_name = out

    # Chu·∫©n b·ªã DataFrame l·ªãch s·ª≠: m·ªói m·ªëc th·ªùi gian = 0/1 (k·∫πt / kh√¥ng)
    df_hist = s.to_frame(name="is_congested")
    df_hist["DateTime"] = df_hist.index
    df_hist["hour"] = df_hist["DateTime"].dt.hour
    df_hist["minute"] = df_hist["DateTime"].dt.minute
    df_hist["weekday"] = df_hist["DateTime"].dt.weekday

    st.subheader("üß≠ Tr·ª£ l√Ω ch·ªçn gi·ªù ƒëi ƒë∆∞·ªùng")

    st.markdown(
        f"D·ª±a tr√™n d·ªØ li·ªáu l·ªãch s·ª≠ c·ªßa tuy·∫øn **{full_name}**, "
        "g·ª£i √Ω khung gi·ªù n√™n ƒëi / n√™n tr√°nh cho **ng√†y h√¥m nay**."
    )

    now = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh")
    today_wd = now.weekday()

    # Ch·ªçn khung gi·ªù quan t√¢m
    window_label = st.selectbox(
        "Ch·ªçn khung gi·ªù b·∫°n quan t√¢m",
        ["S√°ng (06:00‚Äì09:00)", "Chi·ªÅu (16:00‚Äì19:00)"],
        key="hcmc_advisor_window",
    )

    if window_label.startswith("S√°ng"):
        start_hour, end_hour = 6, 9
    else:
        start_hour, end_hour = 16, 19

    # T·∫°o list slot 30' trong kho·∫£ng [start_hour, end_hour)
    slots = []
    h = start_hour
    m = 0
    while h < end_hour:
        slots.append((h, m))
        if m == 0:
            m = 30
        else:
            m = 0
            h += 1

    rows = []
    for (h, m) in slots:
        subset = df_hist[(df_hist["hour"] == h) & (df_hist["minute"] == m)]
        if subset.empty:
            mean_cong = np.nan
        else:
            # ∆Øu ti√™n d√πng ƒë√∫ng th·ª© trong tu·∫ßn h√¥m nay, n·∫øu ƒë·ªß m·∫´u
            subset_today = subset[subset["weekday"] == today_wd]
            if len(subset_today) >= 5:
                mean_cong = subset_today["is_congested"].mean()
            else:
                mean_cong = subset["is_congested"].mean()
        rows.append({"hour": h, "minute": m, "MeanCongestion": mean_cong})

    df_window = pd.DataFrame(rows).dropna(subset=["MeanCongestion"])
    if df_window.empty:
        st.info(
            "Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ t∆∞ v·∫•n khung gi·ªù cho tuy·∫øn n√†y trong kho·∫£ng ƒë√£ ch·ªçn."
        )
        return

    df_window["TimeLabel"] = df_window.apply(
        lambda r: f"{int(r['hour']):02d}:{int(r['minute']):02d}", axis=1
    )
    df_window["CongestionPct"] = (df_window["MeanCongestion"] * 100.0).round(1)

    # ====== T√¨m khung n√™n ƒëi / n√™n tr√°nh theo ng∆∞·ª°ng ph·∫ßn trƒÉm ======
    avg_pct = float(df_window["CongestionPct"].mean())

    GOOD_THR = 30.0  # <= 30%: n√™n ƒëi
    BAD_THR = 70.0   # >= 70%: n√™n tr√°nh

    good = df_window[df_window["CongestionPct"] <= GOOD_THR]
    bad = df_window[df_window["CongestionPct"] >= BAD_THR]

    # Khung n√™n ƒëi: ∆∞u ti√™n t·∫•t c·∫£ khung "good"; n·∫øu kh√¥ng c√≥ th√¨ l·∫•y 1‚Äì2 khung nh·ªè nh·∫•t
    if not good.empty:
        best_list = (
            good.sort_values("CongestionPct")[["TimeLabel"]]
            .drop_duplicates()
            .iloc[:, 0]
            .tolist()
        )
    else:
        best_list = (
            df_window.nsmallest(2, "CongestionPct")[["TimeLabel"]]
            .iloc[:, 0]
            .tolist()
        )

    # Khung n√™n tr√°nh: ∆∞u ti√™n t·∫•t c·∫£ khung "bad"; n·∫øu kh√¥ng c√≥ v√† c√≥ k·∫πt >0% th√¨ l·∫•y 1‚Äì2 khung l·ªõn nh·∫•t
    if not bad.empty:
        worst_list = (
            bad.sort_values("CongestionPct", ascending=False)[["TimeLabel"]]
            .drop_duplicates()
            .iloc[:, 0]
            .tolist()
        )
    else:
        if df_window["CongestionPct"].max() > 0:
            worst_list = (
                df_window.nlargest(2, "CongestionPct")[["TimeLabel"]]
                .iloc[:, 0]
                .tolist()
            )
        else:
            worst_list = []

    best_str = ", ".join(best_list)
    worst_str = ", ".join(worst_list)


    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Khung n√™n ƒëi (√≠t k·∫πt nh·∫•t)",
            best_str or "-",
        )
    with col2:
        st.metric(
            "Khung n√™n tr√°nh (k·∫πt nh·∫•t)",
            worst_str or "-",
        )
    with col3:
        st.metric(
            "M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh",
            f"{avg_pct:,.1f} %",
        )

    # st.markdown(
    #     f"- **Khung n√™n ƒëi**: {best_str if best_str else 'ch∆∞a r√µ do thi·∫øu d·ªØ li·ªáu'}  \n"
    #     f"- **Khung n√™n tr√°nh**: {worst_str if worst_str else 'ch∆∞a r√µ do thi·∫øu d·ªØ li·ªáu'}"
    # )

    # Bi·ªÉu ƒë·ªì c·ªôt m·ª©c ƒë·ªô k·∫πt theo t·ª´ng slot
    chart = (
        alt.Chart(df_window)
        .mark_bar()
        .encode(
            x=alt.X("TimeLabel:N", title="Khung gi·ªù (30 ph√∫t)"),
            y=alt.Y(
                "CongestionPct:Q",
                title="M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh (%)",
            ),
            color=alt.Color(
                "CongestionPct:Q",
                scale=alt.Scale(scheme="RdYlGn_r"),  # th·∫•p = xanh, cao = ƒë·ªè
                legend=alt.Legend(title="K·∫πt xe (%)"),
            ),
            tooltip=[
                alt.Tooltip("TimeLabel:N", title="Khung gi·ªù"),
                alt.Tooltip(
                    "CongestionPct:Q",
                    title="M·ª©c ƒë·ªô k·∫πt xe (%)",
                    format=".1f",
                ),
            ],
        )
        .properties(height=260, title="M·ª©c ƒë·ªô k·∫πt xe trung b√¨nh theo khung 30 ph√∫t")
    )

    st.altair_chart(chart, use_container_width=True)

def render_hcmc_weekly_pattern(route_id: str, routes_geo_all: pd.DataFrame):
    """
    Hi·ªÉn th·ªã 'heatmap' m·∫´u h√¨nh k·∫πt xe theo gi·ªù & th·ª© trong tu·∫ßn
    cho m·ªôt tuy·∫øn HCMC, d·∫°ng b·∫£ng m√†u (pandas.style).
    """
    out = _load_hcmc_series_for_route(route_id, routes_geo_all)
    if out is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    s, full_name, street_name = out

    df = s.to_frame(name="is_congested")
    if df.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    df["DateTime"] = df.index
    df["hour"] = df["DateTime"].dt.hour
    df["weekday"] = df["DateTime"].dt.weekday  # 0=Mon ... 6=Sun

    weekday_map = {
        0: "Th·ª© 2",
        1: "Th·ª© 3",
        2: "Th·ª© 4",
        3: "Th·ª© 5",
        4: "Th·ª© 6",
        5: "Th·ª© 7",
        6: "Ch·ªß nh·∫≠t",
    }
    df["weekday_label"] = df["weekday"].map(weekday_map)

    # Nh√≥m theo (weekday_label, hour) ƒë·ªÉ l·∫•y t·ªâ l·ªá k·∫πt trung b√¨nh
    grp = (
        df.groupby(["weekday_label", "hour"], as_index=False)["is_congested"]
        .mean()
    )
    if grp.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉn th·ªã m·∫´u h√¨nh tu·∫ßn cho tuy·∫øn n√†y.")
        return

    grp["CongestionPct"] = (grp["is_congested"] * 100.0).round(1)
    grp["HourStr"] = grp["hour"].astype(int).astype(str).str.zfill(2) + ":00"

    st.subheader("üìÖ M·∫´u h√¨nh k·∫πt xe trong tu·∫ßn theo gi·ªù")
    st.markdown(
        "M√†u c√†ng ƒë·ªè = tuy·∫øn c√†ng th∆∞·ªùng xuy√™n k·∫πt t·∫°i khung gi·ªù ƒë√≥ "
        "(t√≠nh theo l·ªãch s·ª≠ trong t·∫≠p d·ªØ li·ªáu HCMC)."
    )

    # Pivot th√†nh b·∫£ng 7 x 24 (th·ª© x gi·ªù)
    pivot = grp.pivot_table(
        index="weekday_label",
        columns="HourStr",
        values="CongestionPct",
        aggfunc="mean",
    )

    # S·∫Øp x·∫øp th·ª© theo ƒë√∫ng th·ª© t·ª±
    order_idx = ["Th·ª© 2", "Th·ª© 3", "Th·ª© 4", "Th·ª© 5", "Th·ª© 6", "Th·ª© 7", "Ch·ªß nh·∫≠t"]
    pivot = pivot.reindex(order_idx)

    # S·∫Øp x·∫øp gi·ªù theo th·ª© t·ª± th·ªùi gian
    pivot = pivot.reindex(sorted(pivot.columns), axis=1)

    # ƒê·∫£m b·∫£o gi√° tr·ªã l√† float (NaN chu·∫©n)
    pivot_float = pivot.astype("float")

    # H√†m style ri√™ng cho √¥ kh√¥ng c√≥ d·ªØ li·ªáu
    def style_na(v):
        if pd.isna(v):
            # n·ªÅn tr·∫Øng, ch·ªØ x√°m nh·∫°t (c√≥ th·ªÉ ƒë·ªïi 'No data' t√πy th√≠ch)
            return "background-color: #ffffff; color: #999999;"
        return ""

    styled = (
        pivot_float.style
        # t√¥ heatmap cho c√°c √¥ c√≥ s·ªë
        .background_gradient(cmap="RdYlGn_r", axis=None)
        # format s·ªë, √¥ NaN th√¨ ƒë·ªÉ tr·ªëng ho·∫∑c ghi 'None' t√πy b·∫°n
        .format("{:.1f}", na_rep="None")   # ho·∫∑c na_rep="" n·∫øu mu·ªën √¥ tr·ªëng
        # override l·∫°i style cho √¥ NaN (ƒë·∫∑t sau background_gradient ƒë·ªÉ ƒë√® m√†u)
        .applymap(style_na)
    )

    st.dataframe(styled, use_container_width=True)


# ======================================================
# MAIN APP
# ======================================================
def main():
    if "last_clicked_route_id" not in st.session_state:
        st.session_state["last_clicked_route_id"] = None

    st.set_page_config(page_title="Traffic Forecast App", layout="wide")
    st.title("üö¶ Traffic Forecast App ")

    # Apply pending selection t·ª´ map (tr∆∞·ªõc khi t·∫°o widget)
    if "pending_city" in st.session_state:
        st.session_state["city"] = st.session_state.pop("pending_city")
    if "pending_zone" in st.session_state:
        st.session_state["zone"] = st.session_state.pop("pending_zone")
    if "pending_route" in st.session_state:
        # route t·ª´ map ‚Üí ƒë·ªìng b·ªô tr·ª±c ti·∫øp v√†o widget selectbox "Route"
        st.session_state["route"] = st.session_state.pop("pending_route")

    # ====================================
    # 1) SIDEBAR: CITY / ZONE / ROUTE
    # ====================================

    # ----- CITY -----
    cities = list_cities()
    if "HoChiMinh" not in cities: #TODO :  enhance this later
        cities.append("HoChiMinh")
    if not cities:
        st.error("Kh√¥ng t√¨m th·∫•y city n√†o trong data/processed_ds.")
        return

    CITY_PLACEHOLDER = "(Ch·ªçn city)"
    city_options = [CITY_PLACEHOLDER] + cities

    if "city" not in st.session_state:
        st.session_state["city"] = CITY_PLACEHOLDER

    city_selected = st.sidebar.selectbox(
        "City",
        city_options,
        key="city",
    )

    has_city = city_selected != CITY_PLACEHOLDER
    current_city = city_selected if has_city else None

    # ----- ZONE -----
    if not has_city:
        # Ch∆∞a ch·ªçn city ‚Üí disable zone, d√πng key kh√°c (kh√¥ng ph·∫£i "zone")
        st.sidebar.selectbox(
            "Zone",
            ["(Ch·ªçn city tr∆∞·ªõc)"],
            key="zone_placeholder",
            disabled=True,
        )
        zone = None
        current_zone = None
    else:
        zones = list_zones(current_city)

        # Tr∆∞·ªùng h·ª£p city kh√¥ng c√≥ zone (v√≠ d·ª• HoChiMinh)
        if not zones:
            st.sidebar.selectbox(
                "Zone",
                ["(Kh√¥ng c√≥ zone ‚Äì d√πng to√†n city)"],
                key="zone_info",
                disabled=True,
            )
            zone = None
            current_zone = None
        else:
            # N·∫øu c√≥ nhi·ªÅu zone:
            #  - ƒë∆∞a "(All)" l√™n ƒë·∫ßu
            #  - n·∫øu ch∆∞a c√≥ "(All)" m√† >1 zone ‚Üí th√™m "(All)" v√†o ƒë·∫ßu
            if "(All)" in zones:
                zones = ["(All)"] + [z for z in zones if z != "(All)"]
            elif len(zones) > 1:
                zones = ["(All)"] + zones

            # Default zone:
            #   - N·∫øu c√≥ "(All)" ‚Üí ch·ªçn "(All)"
            #   - N·∫øu ch·ªâ c√≥ 1 zone ‚Üí ch·ªçn ƒë√∫ng zone ƒë√≥
            if "zone" not in st.session_state or st.session_state["zone"] not in zones:
                default_zone = "(All)" if "(All)" in zones else zones[0]
                st.session_state["zone"] = default_zone

            zone = st.sidebar.selectbox(
                "Zone",
                zones,
                key="zone",        # CH·ªà d√πng key="zone" ·ªü ƒë√¢y
                disabled=False,
            )
            current_zone = zone
    # alias cho ph·∫ßn c√≤n l·∫°i c·ªßa code
    city = current_city
    zone = current_zone

    # ====================================
    # 2) LOAD MODEL CONTEXT (FALLBACK zone='(All)')
    # ====================================
    # M·∫∑c ƒë·ªãnh: ch∆∞a c√≥ ctx / model n·∫øu ch∆∞a ch·ªçn city
    ctx = None
    MODEL_GRU = None
    MODEL_RNN = None
    META = None
    SCALER = None
    ROUTES_MODEL = None
    RID2IDX = None
    LOOKBACK = None
    HORIZON = None

    if has_city:
        ctx = None
        zone_for_model = None if zone == "(All)" else zone

        # HoChiMinh: KH√îNG d√πng ModelManager seq2seq (I94/Fremont),
        # m√† d√πng pipeline ri√™ng GRU congestion ‚Üí b·ªè qua
        if city != "HoChiMinh":
            try:
                ctx = get_model_context(city, zone_for_model)
            except FileNotFoundError as e:
                if zone == "(All)":
                    zones_all = list_zones(city)
                    ctx = None
                    for z in zones_all:
                        if z == "(All)":
                            continue
                        try:
                            ctx = load_model_context(city, z)
                            zone_for_model = z
                            break
                        except FileNotFoundError:
                            continue

                    if ctx is None:
                        st.error(str(e))
                        return
                    else:
                        st.info(
                            f"Kh√¥ng c√≥ model t·ªïng cho city={city}, zone='(All)'. "
                            f"ƒêang d√πng model c·ªßa zone='{zone_for_model}'."
                        )
                else:
                    st.error(str(e))
                    return
        else:
            # HoChiMinh kh√¥ng c√≥ ctx seq2seq
            zone_for_model = None
            ctx = None

        # T√°ch context khi ƒë√£ load ƒë∆∞·ª£c ctx (ch·ªâ √°p d·ª•ng cho Minneapolis / Seattle)
        if ctx is not None:
            MODEL_GRU = ctx.gru_model
            MODEL_RNN = getattr(ctx, "rnn_model", None)
            META = ctx.meta
            SCALER = ctx.scaler
            ROUTES_MODEL = ctx.routes_model
            RID2IDX = ctx.rid2idx
            LOOKBACK = ctx.lookback
            HORIZON = ctx.horizon
        else:
            MODEL_GRU = None
            MODEL_RNN = None
            META = None
            SCALER = None
            ROUTES_MODEL = None
            RID2IDX = None
            LOOKBACK = None
            HORIZON = None


    # ====================================
    # 3) ROUTE (sidebar)
    # ====================================
    ROUTE_PLACEHOLDER = "(Ch·ªçn route)"

    # lu√¥n khai b√°o raw_routes, k·ªÉ c·∫£ khi ch∆∞a ch·ªçn city
    raw_routes = []

    if not has_city:
        # Ch∆∞a ch·ªçn city ‚Üí disable route
        route_selected = st.sidebar.selectbox(
            "Route",
            [ROUTE_PLACEHOLDER],
            key="route",
            disabled=True,
        )
        route_id = None
    else:
        if city == "HoChiMinh":
            # HCMC: l·∫•y route t·ª´ routes_geo, hi·ªÉn th·ªã name, value = route_id
            routes_geo_all_sidebar = load_routes_geo().fillna("")
            df_geo_city_sb = routes_geo_all_sidebar[
                routes_geo_all_sidebar["city"] == "HoChiMinh"
            ].copy()

            if df_geo_city_sb.empty:
                st.error("Kh√¥ng t√¨m th·∫•y tuy·∫øn HCMC n√†o trong routes_geo.")
                route_selected = ROUTE_PLACEHOLDER
                route_id = None
            else:
                route_ids = df_geo_city_sb["route_id"].astype(str).tolist()
                id2name = {
                    r["route_id"]: r["name"]
                    for _, r in df_geo_city_sb.iterrows()
                }

                options = [ROUTE_PLACEHOLDER] + route_ids

                if "route" not in st.session_state:
                    st.session_state["route"] = ROUTE_PLACEHOLDER
                elif (
                    st.session_state["route"] != ROUTE_PLACEHOLDER
                    and st.session_state["route"] not in route_ids
                ):
                    st.session_state["route"] = ROUTE_PLACEHOLDER

                route_selected = st.sidebar.selectbox(
                    "Route",
                    options,
                    key="route",
                    format_func=lambda rid: (
                        id2name.get(rid, rid)
                        if rid != ROUTE_PLACEHOLDER
                        else ROUTE_PLACEHOLDER
                    ),
                )
                route_id = None if route_selected == ROUTE_PLACEHOLDER else route_selected
        else:
            # City kh√°c (Minneapolis, Seattle, ...) d√πng route t·ª´ parquet nh∆∞ c≈©
            raw_routes = list_routes(city, None if zone == "(All)" else zone)
            if not raw_routes:
                st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y RouteId n√†o trong parquet cho city/zone n√†y.")
                return

            route_options = [ROUTE_PLACEHOLDER] + raw_routes

            if "route" not in st.session_state:
                st.session_state["route"] = ROUTE_PLACEHOLDER
            elif (
                st.session_state["route"] != ROUTE_PLACEHOLDER
                and st.session_state["route"] not in raw_routes
            ):
                st.session_state["route"] = ROUTE_PLACEHOLDER

            route_selected = st.sidebar.selectbox(
                "Route",
                route_options,
                key="route",
                disabled=False,
            )
            route_id = None if route_selected == ROUTE_PLACEHOLDER else route_selected

    # ====================================
    # 4) TOP-2 MODELS (cho ensemble forecast)
    # ====================================
    top_models = []

    # Ch·ªâ load summary khi ƒë√£ c√≥ ctx + ƒë√£ ch·ªçn route
    if ctx is not None and route_id:
        summary_top2 = load_top2_summary(ctx.family_name, route_id)
        if summary_top2 and "top_models" in summary_top2:
            top_models = summary_top2["top_models"]
        else:
            # fallback: n·∫øu kh√¥ng c√≥ summary, ∆∞u ti√™n GRU r·ªìi RNN
            if ctx.gru_model is not None:
                top_models.append("GRU")
            if getattr(ctx, "rnn_model", None) is not None:
                top_models.append("RNN")

        if not top_models:
            top_models = ["GRU"]
    else:
        # Ch∆∞a ch·ªçn city/route ‚Üí ch∆∞a show g√¨, ch·ªâ map + message "ch·ªçn route"
        pass

    # ----- OPTIONS -----
    tab = st.sidebar.radio("Options", ["FORECAST", "METRICS AND EVALUATION"])

    # ====================================
    # 5) MAP COMPONENT
    # ====================================
    st.subheader("üó∫ Routes Map")

    routes_geo_all = load_routes_geo().fillna("")

    df_geo_city = routes_geo_all[routes_geo_all["city"] == city].copy()
    routes_data = df_geo_city.to_dict("records")
    df_all_geo = routes_geo_all.dropna(subset=["lat", "lon"]).copy()
    all_routes_list = df_all_geo.to_dict("records")

    clicked_route_id = map_routes(
        routes_data=routes_data,
        selected_route_id=route_id,
        all_routes=all_routes_list,
        key="traffic_map",
    )

    if clicked_route_id is not None:
        # Ch·ªâ x·ª≠ l√Ω n·∫øu th·ª±c s·ª± kh√°c l·∫ßn tr∆∞·ªõc
        if clicked_route_id != st.session_state.get("last_clicked_route_id"):
            st.session_state["last_clicked_route_id"] = clicked_route_id

            row = routes_geo_all[
                routes_geo_all["route_id"].str.strip().str.lower()
                == str(clicked_route_id).strip().lower()
                ]

            if not row.empty:
                st.session_state["pending_city"] = row.iloc[0]["city"]
                st.session_state["pending_zone"] = row.iloc[0]["zone"]
                st.session_state["pending_route"] = clicked_route_id
            else:
                st.session_state["pending_route"] = clicked_route_id

            st.rerun()
    if route_id:
        display_name = route_id
        if city == "HoChiMinh":
            row_dn = routes_geo_all[
                (routes_geo_all["city"] == "HoChiMinh")
                & (routes_geo_all["route_id"] == route_id)
            ]
            if not row_dn.empty:
                display_name = row_dn.iloc[0]["name"]
        st.write(f"**ƒêang ch·ªçn tuy·∫øn:** {display_name}")
    else:
        st.write("**Ch∆∞a ch·ªçn tuy·∫øn n√†o**")


    # n·∫øu ch∆∞a c√≥ route th√¨ ch·ªâ show map, kh√¥ng load data/model
    if not route_id:
        st.info("üëÜ H√£y ch·ªçn m·ªôt tuy·∫øn ·ªü sidebar ho·∫∑c click v√†o marker tr√™n b·∫£n ƒë·ªì ƒë·ªÉ xem forecast chi ti·∫øt.")
        return

    # HCMC: d√πng GRU congestion ri√™ng, kh√¥ng d√πng pipeline Vehicles/h nh∆∞ I-94/Fremont
    if city == "HoChiMinh":
        # 1) D·ª± b√°o 2 gi·ªù t·ªõi cho tuy·∫øn ƒëang ch·ªçn
        render_hcmc_congestion_next_2h(route_id, routes_geo_all)

        st.markdown("---")

        # 2) Tr·ª£ l√Ω ch·ªçn gi·ªù ƒëi ƒë∆∞·ªùng (d·ª±a tr√™n l·ªãch s·ª≠)
        render_hcmc_departure_advisor(route_id, routes_geo_all)
        # 3) Heatmap m·∫´u h√¨nh c·∫£ tu·∫ßn
        render_hcmc_weekly_pattern(route_id, routes_geo_all)
        st.markdown("---")
        render_hcmc_eval_summary_for_route(route_id)
        return

    # ====================================
    # 6) LOAD FULL DATA FOR ROUTE
    # ====================================
    df_full = load_slice(
        city=city,
        zone=None if zone == "(All)" else zone,
        routes=[route_id],
        start_dt=None,
        end_dt=None,
    )

    if df_full.empty:
        # st.error("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu history cho route n√†y.")
        return

    df_full = df_full.copy()
    df_full["DateTime"] = pd.to_datetime(df_full["DateTime"], errors="coerce")
    df_full = df_full.dropna(subset=["DateTime"])

    min_dt = df_full["DateTime"].min()
    max_dt = df_full["DateTime"].max()

    # ====================================
    # 7) FORECAST ‚Äì tu·∫ßn k·∫ø ti·∫øp sau d·ªØ li·ªáu g·ªëc (ensemble GRU/RNN)
    # ====================================
    if tab == "FORECAST":
        st.header(" D·ª± ƒëo√°n l∆∞u l∆∞·ª£ng giao th√¥ng cho 7 ng√†y t·ªõi")

        dfs_for_ensemble = []

        for m_name in top_models:
            if m_name not in ("GRU", "RNN", "LSTM"):
                # b·ªè qua model l·∫° (v√≠ d·ª• ARIMA) n·∫øu l·ª° ghi v√†o JSON
                continue

            if m_name in ("GRU", "RNN"):
                # logic c≈©: d√πng forecast_week_after_last_point v·ªõi GRU/RNN
                df_m, anchor_m = forecast_week_after_last_point(
                    route_id=route_id,
                    city=city,
                    zone=None if zone == "(All)" else zone,
                    ctx=ctx,
                    n_days=7,
                    model_type=m_name,
                )
            elif m_name == "LSTM":
                # NEW: forecast tu·∫ßn b·∫±ng LSTM ri√™ng
                df_m, anchor_m = forecast_week_after_last_point_lstm(
                    route_id=route_id,
                    city=city,
                    zone=None if zone == "(All)" else zone,
                    ctx=ctx,
                    n_days=7,
                )
            else:
                df_m, anchor_m = None, None

            if df_m is not None and not df_m.empty:
                dfs_for_ensemble.append((m_name, df_m, anchor_m))


        if not dfs_for_ensemble:
            st.warning("Kh√¥ng forecast ƒë∆∞·ª£c b·∫±ng GRU/RNN top-2, fallback GRU.")
            df_fc_raw, anchor_day_raw = forecast_week_after_last_point(
                route_id=route_id,
                city=city,
                zone=None if zone == "(All)" else zone,
                ctx=ctx,
                n_days=7,
                model_type="GRU",
            )
            if df_fc_raw is not None and not df_fc_raw.empty:
                df_fc_raw = df_fc_raw.copy()
                df_fc_raw["DateTime"] = pd.to_datetime(
                    df_fc_raw["DateTime"], errors="coerce"
                )
                df_fc_raw = df_fc_raw.dropna(subset=["DateTime"])
                df_fc_raw = df_fc_raw.rename(
                    columns={"PredictedVehicles": "Pred_GRU"}
                )
                df_fc_raw["Pred_ENSEMBLE"] = df_fc_raw["Pred_GRU"]
                df_fc_raw["PredictedVehicles"] = df_fc_raw["Pred_ENSEMBLE"]
        else:
            anchor_day_raw = dfs_for_ensemble[0][2]

            df_merge = None
            for m_name, df_m, _ in dfs_for_ensemble:
                col = f"Pred_{m_name}"
                tmp = (
                    df_m[["DateTime", "PredictedVehicles"]]
                    .rename(columns={"PredictedVehicles": col})
                )
                df_merge = tmp if df_merge is None else df_merge.merge(
                    tmp, on="DateTime", how="inner"
                )

            if df_merge is not None and not df_merge.empty:
                model_pred_cols = [
                    f"Pred_{m}" for m in top_models if f"Pred_{m}" in df_merge.columns
                ]
                if model_pred_cols:
                    df_merge["Pred_ENSEMBLE"] = df_merge[model_pred_cols].mean(
                        axis=1
                    )
                else:
                    df_merge["Pred_ENSEMBLE"] = np.nan

                df_merge["PredictedVehicles"] = df_merge["Pred_ENSEMBLE"]
                df_fc_raw = df_merge.copy()
            else:
                df_fc_raw = None

        if df_fc_raw is None or df_fc_raw.empty:
            st.warning("Kh√¥ng forecast ƒë∆∞·ª£c (thi·∫øu d·ªØ li·ªáu history).")
        else:
            df_fc = df_fc_raw.copy()
            df_fc["DateTime"] = pd.to_datetime(df_fc["DateTime"], errors="coerce")
            df_fc = df_fc.dropna(subset=["DateTime"])

            days = (
                df_fc["DateTime"]
                .dt.normalize()
                .drop_duplicates()
                .sort_values()
                .tolist()
            )

            if days:
                day_tabs = st.tabs([vn_weekday_label(d) for d in days])

                for d, t in zip(days, day_tabs):
                    with t:
                        day_start = d
                        day_end = d + pd.Timedelta(days=1)

                        df_day = df_fc[
                            (df_fc["DateTime"] >= day_start)
                            & (df_fc["DateTime"] < day_end)
                        ].copy()

                        if df_day.empty:
                            st.info("Kh√¥ng c√≥ forecast cho ng√†y n√†y.")
                            continue

                        # C·ªôt d√πng ƒë·ªÉ ph√¢n t√≠ch: ∆∞u ti√™n ensemble
                        metric_col = "PredictedVehicles_Ensemble"
                        if metric_col not in df_day.columns:
                            metric_col = "PredictedVehicles"

                        df_day["DateTime"] = pd.to_datetime(
                            df_day["DateTime"], errors="coerce"
                        )
                        df_day = df_day.dropna(subset=["DateTime"])

                        s = (
                            df_day.set_index("DateTime")[metric_col]
                            .astype(float)
                            .sort_index()
                        )

                        if s.empty:
                            st.info("Kh√¥ng c√≥ d·ªØ li·ªáu forecast h·ª£p l·ªá cho ng√†y n√†y.")
                            continue

                        # === Ph√¢n t√≠ch gi·ªù cao ƒëi·ªÉm / v·∫Øng nh·∫•t / trung b√¨nh ===
                        peak_time = s.idxmax()
                        peak_val = float(s.max())

                        low_time = s.idxmin()
                        low_val = float(s.min())

                        avg_val = float(s.mean())
                        st.markdown("### üìà Ph√¢n t√≠ch nhanh trong ng√†y")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric(
                                "Gi·ªù cao ƒëi·ªÉm nh·∫•t",
                                f"{peak_time:%H:%M}",
                                help=f"Kho·∫£ng {peak_val:,.0f} vehicles/h",
                            )
                        with col2:
                            st.metric(
                                "Gi·ªù v·∫Øng nh·∫•t",
                                f"{low_time:%H:%M}",
                                help=f"Kho·∫£ng {low_val:,.0f} vehicles/h",
                            )
                        with col3:
                            st.metric(
                                "L∆∞u l∆∞·ª£ng trung b√¨nh",
                                f"{avg_val:,.0f} xe/gi·ªù",
                            )
                        # B·∫£ng ngang
                        st.markdown("### üßÆ L∆∞u l∆∞·ª£ng xe c·ªô theo gi·ªù")

                        # s: Series index = DateTime, value = Vehicles/h (ensemble)
                        s_label = s.copy()
                        s_label.index = s_label.index.strftime("%H:%M")
                        s_label_int = s_label.round(0).astype("Int64")  # convert to int, nullable

                        # 1 d√≤ng, c√°c c·ªôt l√† gi·ªù
                        tbl = s_label_int.to_frame().T
                        tbl.index = ["Vehicles/h"]

                        styled_tbl = (
                            tbl.style
                            .format("{:,.0f}", na_rep="-")  # hi·ªÉn th·ªã int, c√≥ ph√¢n c√°ch
                            .background_gradient(axis=1, cmap="YlOrRd")  # th·∫•p = v√†ng nh·∫°t, cao = ƒë·ªè
                            .highlight_max(axis=1, color="#7f0000   ")  # gi·ªù cao ƒëi·ªÉm nh·∫•t t√¥ ƒë·ªè h·∫≥n
                        )

                        st.dataframe(styled_tbl, use_container_width=True, height=70)
                        # st.dataframe(styled_tbl, use_container_width=True, height=140)

                        # Ch√∫ gi·∫£i m√†u
                        st.markdown(
                            """
                            <div style="font-size:0.9rem; margin-top:4px;">
                              <b>Ch√∫ th√≠ch m√†u:</b>
                              <span style="display:inline-block;width:14px;height:14px;background-color:#008000;border-radius:3px;margin:0 4px 0 8px;border:1px solid #ccc;"></span>
                              Xanh l√°  = l∆∞u l∆∞·ª£ng th·∫•p / th∆∞a th·ªõt
                              <span style="display:inline-block;width:14px;height:14px;background-color:#FFD700;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
                              V√†ng = trung b√¨nh
                              <span style="display:inline-block;width:14px;height:14px;background-color:#CC0000;border-radius:3px;margin:0 4px 0 12px;border:1px solid #ccc;"></span>
                              ƒê·ªè = gi·ªù r·∫•t ƒë√¥ng (cao ƒëi·ªÉm)
                            </div>
                            """,
                            unsafe_allow_html=True,
                        )
                        st.markdown(f"**M√¥ h√¨nh s·ª≠ d·ª•ng:** {', '.join(top_models)}")

                        # Tooltip hi·ªÉn th·ªã t·ª´ng model n·∫øu c√≥
                        tooltip_fields = [
                            alt.Tooltip("DateTime:T", title="Th·ªùi gian"),
                            alt.Tooltip(
                                "Pred_ENSEMBLE:Q",
                                title="D·ª± b√°o ensemble",
                                format=".0f",
                            ),
                        ]
                        if "Pred_GRU" in df_day.columns:
                            tooltip_fields.append(
                                alt.Tooltip("Pred_GRU:Q", title="GRU", format=".0f")
                            )
                        if "Pred_RNN" in df_day.columns:
                            tooltip_fields.append(
                                alt.Tooltip("Pred_RNN:Q", title="RNN", format=".0f")
                            )
                        if "Pred_LSTM" in df_day.columns:
                            tooltip_fields.append(
                                alt.Tooltip("Pred_LSTM:Q", title="LSTM", format=".0f")
                            )

                        tooltip_fields.append(
                            alt.Tooltip(
                                "PredictedVehicles:Q",
                                title="Ensemble (avg)",
                                format=".0f",
                            )
                        )

                        df_day = df_day.copy()

                        q_low = df_day["PredictedVehicles"].quantile(0.2)
                        q_high = df_day["PredictedVehicles"].quantile(0.8)

                        def level_label(v):
                            if v >= q_high:
                                return "R·∫•t ƒë√¥ng"
                            elif v <= q_low:
                                return "Th∆∞a th·ªõt"
                            else:
                                return "Trung b√¨nh"

                        df_day["TrafficLevel"] = df_day["PredictedVehicles"].apply(level_label)

                        base = alt.Chart(df_day).encode(
                            x=alt.X("DateTime:T", title="Th·ªùi gian")
                        )

                        line = base.mark_line(color="lightgray").encode(
                            y=alt.Y("PredictedVehicles:Q", title="Vehicles"),
                        )
                        color_scale = alt.Scale(
                            domain=["Th∆∞a th·ªõt", "Trung b√¨nh", "R·∫•t ƒë√¥ng"],
                            range=["#008000", "#0000ff", "#CC0000"],
                        )
                        points = base.mark_point(size=70).encode(
                            y="PredictedVehicles:Q",
                            color=alt.Color(
                                "TrafficLevel:N",
                                scale=color_scale,
                                legend=alt.Legend(title="M·ª©c l∆∞u l∆∞·ª£ng"),
                            ),
                            tooltip=tooltip_fields,
                        )

                        chart = (line + points).interactive().properties(
                            height=320,
                            title=f"D·ª± b√°o cho {vn_weekday_label(day_start)}",
                        )
                        st.altair_chart(chart, use_container_width=True)
            else:
                st.info("Kh√¥ng c√≥ ng√†y n√†o trong forecast.")

    # ====================================
    # 8) DAILY TRAFFIC ‚Äì 3 TH√ÅNG G·∫¶N NH·∫§T
    #     Actual vs GRU / RNN / LSTM / ARIMA / SARIMA + Metrics t·ªïng 3 th√°ng
    # ====================================
    elif tab == "METRICS AND EVALUATION":
        st.header("üìö Th·ªëng k√™ v√† ƒë√°nh gi√°")

        # ƒê·ªçc cache do script precompute_daily_3months.py sinh ra:
        #   model/<family_name>/<route_id>_daily_3months.parquet
        cache_dir = Path("model") / ctx.family_name
        cache_path = cache_dir / f"{route_id}_daily_3months.parquet"

        if not cache_path.exists():
            st.info(
                f"‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y file cache daily: {cache_path}. "
                "H√£y ch·∫°y scripts/precompute_daily_3months.py tr∆∞·ªõc, ho·∫∑c b·∫≠t l·∫°i ch·∫ø ƒë·ªô t√≠nh tr·ª±c ti·∫øp trong app."
            )
            return

        try:
            df_eval = pd.read_parquet(cache_path)
        except Exception as ex:
            st.error(f"L·ªói ƒë·ªçc file cache daily: {ex}")
            return

        if df_eval is None or df_eval.empty:
            st.info("File cache daily tr·ªëng, kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã.")
            return

        # Chu·∫©n h√≥a c·ªôt Date
        if "Date" not in df_eval.columns or "DailyActual" not in df_eval.columns:
            st.warning(
                "File cache daily kh√¥ng c√≥ ƒë·ªß c·ªôt 'Date' / 'DailyActual'. Ki·ªÉm tra l·∫°i file precompute."
            )
            return

        df_eval = df_eval.copy()
        df_eval["Date"] = pd.to_datetime(df_eval["Date"]).dt.normalize()
        # ----------------------------------------------------------
        # Fallback: n·∫øu cache ch∆∞a c√≥ Daily_ARIMA / Daily_SARIMA,
        # nh∆∞ng app import ƒë∆∞·ª£c ARIMA/SARIMA th√¨ t√≠nh b·ªï sung t·∫°i ch·ªó.
        # ----------------------------------------------------------
        dates = df_eval["Date"].dropna().drop_duplicates().sort_values().tolist()

        # ---- Fallback ARIMA ----
        if HAS_ARIMA and forecast_arima_for_day is not None and "Daily_ARIMA" not in df_eval.columns:
            records = []
            for d in dates:
                day_start = pd.Timestamp(d).normalize()
                day_end = day_start + pd.Timedelta(days=1)

                try:
                    # theo fix tr∆∞·ªõc ƒë√¢y: forecast_arima_for_day(df_full, day_start)
                    out = forecast_arima_for_day(df_full, day_start)
                    if isinstance(out, tuple):
                        df_fc_arima = out[0]
                    else:
                        df_fc_arima = out
                except Exception as ex:
                    print(f"[Daily-ARIMA] error {day_start.date()}: {ex}")
                    continue

                if df_fc_arima is None or df_fc_arima.empty:
                    continue

                df_a = df_fc_arima.copy()
                df_a["DateTime"] = pd.to_datetime(df_a["DateTime"], errors="coerce")
                df_a = df_a.dropna(subset=["DateTime"])
                df_a = df_a[
                    (df_a["DateTime"] >= day_start)
                    & (df_a["DateTime"] < day_end)
                    ]
                if df_a.empty:
                    continue

                # tu·ª≥ arima_utils: ∆∞u ti√™n Pred_ARIMA, fallback PredictedVehicles
                pred_col = "Pred_ARIMA" if "Pred_ARIMA" in df_a.columns else "PredictedVehicles"
                if pred_col not in df_a.columns:
                    continue

                v = float(df_a[pred_col].sum())
                records.append({"Date": day_start, "DailyPred": v})

            if records:
                df_arima = (
                    pd.DataFrame(records)
                    .groupby("Date", as_index=False)["DailyPred"]
                    .mean()
                    .rename(columns={"DailyPred": "Daily_ARIMA"})
                )
                df_eval = df_eval.merge(df_arima, on="Date", how="left")

        # ---- Fallback SARIMA ----
        if HAS_SARIMA and forecast_sarima_for_day is not None and "Daily_SARIMA" not in df_eval.columns:
            records = []
            for d in dates:
                day_start = pd.Timestamp(d).normalize()
                day_end = day_start + pd.Timedelta(days=1)

                try:
                    out = forecast_sarima_for_day(df_full, day_start)
                    if isinstance(out, tuple):
                        df_fc_sarima = out[0]
                    else:
                        df_fc_sarima = out
                except Exception as ex:
                    print(f"[Daily-SARIMA] error {day_start.date()}: {ex}")
                    continue

                if df_fc_sarima is None or df_fc_sarima.empty:
                    continue

                df_s = df_fc_sarima.copy()
                df_s["DateTime"] = pd.to_datetime(df_s["DateTime"], errors="coerce")
                df_s = df_s.dropna(subset=["DateTime"])
                df_s = df_s[
                    (df_s["DateTime"] >= day_start)
                    & (df_s["DateTime"] < day_end)
                    ]
                if df_s.empty:
                    continue

                pred_col = "Pred_SARIMA" if "Pred_SARIMA" in df_s.columns else "PredictedVehicles"
                if pred_col not in df_s.columns:
                    continue

                v = float(df_s[pred_col].sum())
                records.append({"Date": day_start, "DailyPred": v})

            if records:
                df_sarima = (
                    pd.DataFrame(records)
                    .groupby("Date", as_index=False)["DailyPred"]
                    .mean()
                    .rename(columns={"DailyPred": "Daily_SARIMA"})
                )
                df_eval = df_eval.merge(df_sarima, on="Date", how="left")

        # ---- Tab ----
        tab_cmp_daily, tab_cmp_weekly, tab_cmp_monthly = st.tabs(["Daily", "Weekly", "Monthly"])

        # -----------------
        # 7.1 Tab Daily
        # -----------------
        with tab_cmp_daily:
            st.subheader("DAILY (Actual + Models) ‚Äì 3 th√°ng g·∫ßn nh·∫•t")
            # ==== Chart multi-line (Actual + Models) ====
            frames = [
                df_eval[["Date", "DailyActual"]]
                .rename(columns={"DailyActual": "DailyValue"})
                .assign(Source="Actual")
            ]

            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Daily_{m_name}"
                if col_name in df_eval.columns and df_eval[col_name].notna().any():
                    frames.append(
                        df_eval[["Date", col_name]]
                        .rename(columns={col_name: "DailyValue"})
                        .assign(Source=m_name)
                    )

            if frames:
                df_chart = pd.concat(frames, ignore_index=True)
                df_chart = df_chart.sort_values("Date")

                chart_daily = (
                    alt.Chart(df_chart)
                    .mark_line(point=True)
                    .encode(
                        x=alt.X("Date:T", title="Date"),
                        y=alt.Y("DailyValue:Q", title="Vehicles / day"),
                        color=alt.Color("Source:N", title="Series"),
                        tooltip=[
                            alt.Tooltip("Date:T", title="Date"),
                            alt.Tooltip("Source:N", title="Series"),
                            alt.Tooltip("DailyValue:Q", title="Vehicles/day", format=","),
                        ],
                    )
                    .properties(height=300)
                )
                st.altair_chart(chart_daily, use_container_width=True)

                with st.expander(
                        "üîç Xem b·∫£ng daily (Actual + Models) ‚Äì 3 th√°ng g·∫ßn nh·∫•t"
                ):
                    df_show = df_eval.copy()
                    for c in df_show.columns:
                        if c.startswith("Daily"):
                            df_show[c] = df_show[c].round().astype("Int64").apply(lambda x: f"{x:,.0f}")
                    st.dataframe(df_show.sort_values("Date"), use_container_width=True)
            else:
                st.info("Kh√¥ng c√≥ series n√†o (GRU/RNN/LSTM/ARIMA/SARIMA) ƒë·ªÉ hi·ªÉn th·ªã.")

            # ==== Metrics t·ªïng 3 th√°ng cho t·ª´ng model (n·∫øu c√≥ c·ªôt) ====
            metrics_rows = []
            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Daily_{m_name}"
                if col_name not in df_eval.columns:
                    continue
                valid = df_eval[["DailyActual", col_name]].dropna()
                if valid.empty:
                    continue

                actual = valid["DailyActual"].values.astype(float)
                pred = valid[col_name].values.astype(float)

                mse = mean_squared_error(actual, pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(actual, pred)

                if np.any(actual != 0):
                    mape = (
                            np.mean(
                                np.abs((actual - pred)[actual != 0] / actual[actual != 0])
                            )
                            * 100.0
                    )
                else:
                    mape = np.nan

                denom = np.abs(actual) + np.abs(pred)
                smape = (
                        np.mean(
                            2.0 * np.abs(pred - actual) / np.where(denom == 0, 1.0, denom)
                        )
                        * 100.0
                )

                r2 = r2_score(actual, pred)

                metrics_rows.append(
                    {
                        "Model": m_name,
                        "MSE": mse,
                        "RMSE": rmse,
                        "MAE": mae,
                        "MAPE (%)": mape,
                        "SMAPE (%)": smape,
                        "R¬≤": r2,
                    }
                )

            if metrics_rows:
                st.subheader(" ƒê√°nh gi√° sai s·ªë theo t·ª´ng model trong 3 th√°ng g·∫ßn nh·∫•t")
                df_metrics = pd.DataFrame(metrics_rows)
                for c in ["MSE", "RMSE", "MAE"]:
                    df_metrics[c] = df_metrics[c].round(2)
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_metrics[c] = df_metrics[c].round(3)

                # ---- Format s·ªë theo d·∫°ng 000,000,000.00 ----
                format_cols = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
                df_formatted = df_metrics.copy()
                for c in ["MSE", "RMSE", "MAE"]:
                    df_formatted[c] = df_formatted[c].apply(lambda x: f"{x:,.2f}")
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_formatted[c] = df_formatted[c].apply(lambda x: f"{x:,.3f}")

                st.dataframe(df_formatted, use_container_width=True)

            # ==== Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° ====
            st.subheader("üìä Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° sai s·ªë")
            metrics_list = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
            cols = st.columns(2) # T·∫°o layout 2 c·ªôt

            for i, metric in enumerate(metrics_list):
                chart = (
                    alt.Chart(df_metrics)
                    .mark_bar()
                    .encode(
                        x=alt.X("Model:N", title="Model", axis=alt.Axis(labelAngle=0)),
                        y=alt.Y(f"{metric}:Q", title=metric),
                        tooltip=["Model", metric]
                    )
                    .properties(
                        height=300,
                        title=alt.TitleParams(
                            f"{metric}",
                            fontSize=24,
                            fontWeight="bold",
                            color="#333",
                            anchor="middle"  # cƒÉn gi·ªØa
                        )
                    )
                )

                # V·∫Ω ƒë√∫ng c·ªôt (0 ho·∫∑c 1)
                cols[i % 2].altair_chart(chart, use_container_width=True)

                # Sau m·ªói 2 bi·ªÉu ƒë·ªì ‚Üí t·∫°o h√†ng m·ªõi
                if i % 2 == 1 and i < len(metrics_list) - 1:
                    cols = st.columns(2)

        # -----------------
        # 7.2 Tab Weekly
        # -----------------
        with tab_cmp_weekly:

            df_weekly = df_eval.copy()
            df_weekly["Date"] = pd.to_datetime(df_weekly["Date"])

            # Convert th√†nh tu·∫ßn
            df_weekly["WeekStart"] = df_weekly["Date"].dt.to_period("W").apply(lambda r: r.start_time)
            df_weekly["WeekEnd"] = df_weekly["Date"].dt.to_period("W").apply(lambda r: r.end_time)

            # Gom weekly (sum cho traffic)
            # L·∫•y to√†n b·ªô c·ªôt Daily_*
            daily_cols = [c for c in df_weekly.columns if c.startswith("Daily")]

            # T·∫°o dict ƒë·ªông cho agg
            agg_dict = {c: "sum" for c in daily_cols}

            # Group
            df_weekly = (
                df_weekly.groupby(["WeekStart", "WeekEnd"])
                .agg(agg_dict)
                .reset_index()
            )

            # ƒê·ªïi t√™n c·ªôt Daily* ‚Üí Weekly*
            df_weekly = df_weekly.rename(
                columns={c: c.replace("Daily", "Weekly") for c in daily_cols}
            )

            # Format range: YYYY-MM-DD ‚Üí YYYY-MM-DD
            df_weekly["WeekRange"] = df_weekly["WeekStart"].dt.strftime("%Y-%m-%d") + " ‚Üí " + \
                                     df_weekly["WeekEnd"].dt.strftime("%Y-%m-%d")

            # ==== Chart multi-line Weekly (Actual + Models) ====
            st.subheader("WEEKLY (Actual + Models) ‚Äì 3 th√°ng g·∫ßn nh·∫•t")

            frames = [
                df_weekly[["WeekStart", "WeeklyActual"]]
                .rename(columns={"WeeklyActual": "WeeklyValue"})
                .assign(Source="Actual")
            ]

            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Weekly_{m_name}"
                if col_name in df_weekly.columns and df_weekly[col_name].notna().any():
                    frames.append(
                        df_weekly[["WeekStart", col_name]]
                        .rename(columns={col_name: "WeeklyValue"})
                        .assign(Source=m_name)
                    )

            if frames:
                df_chart_w = pd.concat(frames, ignore_index=True)
                df_chart_w = df_chart_w.sort_values("WeekStart")

                chart_weekly = (
                    alt.Chart(df_chart_w)
                    .mark_line(point=True)
                    .encode(
                        x=alt.X("WeekStart:T", title="Week (Start Date)"),
                        y=alt.Y("WeeklyValue:Q", title="Vehicles / week"),
                        color=alt.Color("Source:N", title="Series"),
                        tooltip=[
                            alt.Tooltip("WeekStart:T", title="Week Start"),
                            alt.Tooltip("Source:N", title="Series"),
                            alt.Tooltip("WeeklyValue:Q", title="Vehicles/week", format=","),
                        ],
                    )
                    .properties(height=300)
                )
                st.altair_chart(chart_weekly, use_container_width=True)

                with st.expander("Xem b·∫£ng Weekly (Actual + Models) ‚Äì t·ªïng h·ª£p theo tu·∫ßn"):
                    df_weekly_show = df_weekly.copy()
                    for c in df_weekly_show.columns:
                        if c.startswith("Weekly"):
                            df_weekly_show[c] = df_weekly_show[c].round().astype("Int64").apply(lambda x: f"{x:,.0f}")
                    st.dataframe(
                        df_weekly_show[["WeekRange"] +
                                  [c for c in df_weekly_show.columns if
                                   c not in ["Date", "Year", "Week", "WeekRange", "WeekStart", "WeekEnd"]]],
                        use_container_width=True
                    )
            else:
                st.info("Kh√¥ng c√≥ series n√†o (GRU/RNN/LSTM/ARIMA/SARIMA) ƒë·ªÉ hi·ªÉn th·ªã.")

            # ==== Metrics t·ªïng Weekly cho t·ª´ng model (n·∫øu c√≥ c·ªôt) ====
            metrics_rows = []

            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Weekly_{m_name}"
                if col_name not in df_weekly.columns:
                    continue

                valid = df_weekly[["WeeklyActual", col_name]].dropna()
                if valid.empty:
                    continue

                actual = valid["WeeklyActual"].values.astype(float)
                pred = valid[col_name].values.astype(float)

                # Sai s·ªë
                mse = mean_squared_error(actual, pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(actual, pred)

                mape = (
                        np.mean(np.abs((actual - pred) / np.where(actual == 0, np.nan, actual))) * 100
                )

                denom = np.abs(actual) + np.abs(pred)
                smape = (
                        np.mean(2.0 * np.abs(pred - actual) / np.where(denom == 0, 1.0, denom)) * 100
                )

                r2 = r2_score(actual, pred)

                metrics_rows.append(
                    {
                        "Model": m_name,
                        "MSE": mse,
                        "RMSE": rmse,
                        "MAE": mae,
                        "MAPE (%)": mape,
                        "SMAPE (%)": smape,
                        "R¬≤": r2,
                    }
                )

            if metrics_rows:
                st.subheader("ƒê√°nh gi√° sai s·ªë theo t·ª´ng model ‚Äì d·ªØ li·ªáu Weekly (3 th√°ng g·∫ßn nh·∫•t)")
                df_metrics_weekly = pd.DataFrame(metrics_rows)

                for c in ["MSE", "RMSE", "MAE"]:
                    df_metrics_weekly[c] = df_metrics_weekly[c].round(2)
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_metrics_weekly[c] = df_metrics_weekly[c].round(3)

                # ---- Format s·ªë theo d·∫°ng 000,000,000.00 ----
                format_cols = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
                df_formatted_weekly = df_metrics_weekly.copy()
                for c in ["MSE", "RMSE", "MAE"]:
                    df_formatted_weekly[c] = df_formatted_weekly[c].apply(lambda x: f"{x:,.2f}")
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_formatted_weekly[c] = df_formatted_weekly[c].apply(lambda x: f"{x:,.3f}")

                st.dataframe(df_formatted_weekly, use_container_width=True)

            else:
                st.info("Kh√¥ng c√≥ d·ªØ li·ªáu Weekly ƒë·ªÉ t√≠nh metrics.")

            # ==== Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° ====
            st.subheader("üìä Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° sai s·ªë")
            metrics_list = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
            cols = st.columns(2)  # 2 c·ªôt m·ªói h√†ng

            for i, metric in enumerate(metrics_list):
                chart = (
                    alt.Chart(df_metrics_weekly)
                    .mark_bar()
                    .encode(
                        x=alt.X("Model:N", title="Model", axis=alt.Axis(labelAngle=0)),
                        y=alt.Y(f"{metric}:Q", title=metric),
                        tooltip=["Model", metric]
                    )
                    .properties(
                        height=300,
                        title=alt.TitleParams(
                            f"{metric}",
                            fontSize=24,
                            fontWeight="bold",
                            color="#333",
                            anchor="middle"  # cƒÉn gi·ªØa
                        )
                    )
                )

                # v·∫Ω v√†o ƒë√∫ng c·ªôt
                cols[i % 2].altair_chart(chart, use_container_width=True)

                # t·∫°o h√†ng k·∫ø ti·∫øp sau m·ªói 2 chart
                if i % 2 == 1 and i < len(metrics_list) - 1:
                    cols = st.columns(2)

        # -----------------
        # 7.2 Tab Monthly
        # -----------------
        with tab_cmp_monthly:
            df_monthly = df_eval.copy()
            df_monthly["Date"] = pd.to_datetime(df_monthly["Date"])

            # Convert th√†nh th√°ng
            df_monthly["MonthStart"] = df_monthly["Date"].dt.to_period("M").apply(lambda r: r.start_time)
            df_monthly["MonthEnd"] = df_monthly["Date"].dt.to_period("M").apply(lambda r: r.end_time)

            # Gom monthly (sum cho traffic)
            daily_cols = [c for c in df_monthly.columns if c.startswith("Daily")]

            agg_dict = {c: "sum" for c in daily_cols}

            df_monthly = (
                df_monthly.groupby(["MonthStart", "MonthEnd"])
                .agg(agg_dict)
                .reset_index()
            )

            # ƒê·ªïi t√™n Daily* ‚Üí Monthly*
            df_monthly = df_monthly.rename(
                columns={c: c.replace("Daily", "Monthly") for c in daily_cols}
            )

            # Hi·ªÉn th·ªã d·∫°ng "YYYY-MM-DD ‚Üí YYYY-MM-DD"
            df_monthly["MonthRange"] = (
                    df_monthly["MonthStart"].dt.strftime("%Y-%m-%d") +
                    " ‚Üí " +
                    df_monthly["MonthEnd"].dt.strftime("%Y-%m-%d")
            )

            # ==== Chart multi-line Monthly (Actual + Models) ====
            st.subheader("MONTHLY (Actual + Models) ‚Äì 3 th√°ng g·∫ßn nh·∫•t")

            frames_m = [
                df_monthly[["MonthStart", "MonthlyActual"]]
                .rename(columns={"MonthlyActual": "MonthlyValue"})
                .assign(Source="Actual")
            ]

            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Monthly_{m_name}"
                if col_name in df_monthly.columns and df_monthly[col_name].notna().any():
                    frames_m.append(
                        df_monthly[["MonthStart", col_name]]
                        .rename(columns={col_name: "MonthlyValue"})
                        .assign(Source=m_name)
                    )

            df_chart_m = pd.concat(frames_m, ignore_index=True).sort_values("MonthStart")

            chart_monthly = (
                alt.Chart(df_chart_m)
                .mark_line(point=True)
                .encode(
                    x=alt.X("MonthStart:T", title="Month (Start Date)"),
                    y=alt.Y("MonthlyValue:Q", title="Vehicles / month"),
                    color=alt.Color("Source:N"),
                    tooltip=[
                        alt.Tooltip("MonthStart:T", title="Month Start"),
                        alt.Tooltip("Source:N", title="Series"),
                        alt.Tooltip("MonthlyValue:Q", format=","),
                    ],
                )
                .properties(height=300)
            )

            st.altair_chart(chart_monthly, use_container_width=True)

            with st.expander("Xem b·∫£ng Monthly (Actual + Models)"):
                df_monthly_show = df_monthly.copy()
                for c in df_monthly_show.columns:
                    if c.startswith("Monthly"):
                        df_monthly_show[c] = df_monthly_show[c].round().astype("Int64").apply(lambda x: f"{x:,.0f}")
                st.dataframe(
                    df_monthly_show[
                        ["MonthRange"] +
                        [c for c in df_monthly_show.columns if c not in
                         ["Date", "MonthStart", "MonthEnd", "MonthRange"]]
                        ],
                    use_container_width=True
                )

            # ==== Metrics t·ªïng Weekly cho t·ª´ng model (n·∫øu c√≥ c·ªôt) ====
            metrics_rows = []

            for m_name in ["GRU", "RNN", "LSTM", "ARIMA", "SARIMA"]:
                col_name = f"Monthly_{m_name}"
                if col_name not in df_monthly.columns:
                    continue

                valid = df_monthly[["MonthlyActual", col_name]].dropna()
                if valid.empty:
                    continue

                actual = valid["MonthlyActual"].values.astype(float)
                pred = valid[col_name].values.astype(float)

                mse = mean_squared_error(actual, pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(actual, pred)
                mape = np.mean(np.abs((actual - pred) / np.where(actual == 0, np.nan, actual))) * 100

                denom = np.abs(actual) + np.abs(pred)
                smape = np.mean(2 * np.abs(pred - actual) / np.where(denom == 0, 1, denom)) * 100

                r2 = r2_score(actual, pred)

                metrics_rows.append({
                    "Model": m_name,
                    "MSE": mse,
                    "RMSE": rmse,
                    "MAE": mae,
                    "MAPE (%)": mape,
                    "SMAPE (%)": smape,
                    "R¬≤": r2
                })

            if metrics_rows:
                st.subheader("ƒê√°nh gi√° sai s·ªë theo t·ª´ng model ‚Äì d·ªØ li·ªáu Monthly")
                df_metrics_monthly = pd.DataFrame(metrics_rows)
                for c in ["MSE", "RMSE", "MAE"]:
                    df_metrics_monthly[c] = df_metrics_monthly[c].round(2)
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_metrics_monthly[c] = df_metrics_monthly[c].round(3)

                # ---- Format s·ªë theo d·∫°ng 000,000,000.00 ----
                format_cols = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
                df_formatted_monthly = df_metrics_monthly.copy()
                for c in ["MSE", "RMSE", "MAE"]:
                    df_formatted_monthly[c] = df_formatted_monthly[c].apply(lambda x: f"{x:,.2f}")
                for c in ["MAPE (%)", "SMAPE (%)", "R¬≤"]:
                    df_formatted_monthly[c] = df_formatted_monthly[c].apply(lambda x: f"{x:,.3f}")

                st.dataframe(df_formatted_monthly, use_container_width=True)

            # ==== Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° ====
            st.subheader("üìä Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ƒë√°nh gi√° sai s·ªë")
            metrics_list = ["MSE", "RMSE", "MAE", "MAPE (%)", "SMAPE (%)", "R¬≤"]
            cols = st.columns(2)

            for i, metric in enumerate(metrics_list):
                chart = (
                    alt.Chart(df_metrics_monthly)
                    .mark_bar()
                    .encode(
                        x=alt.X("Model:N", title="Model", axis=alt.Axis(labelAngle=0)),
                        y=alt.Y(f"{metric}:Q", title=metric),
                        tooltip=["Model", metric]
                    )
                    .properties(
                        height=300,
                        title=alt.TitleParams(
                            f"{metric}",
                            fontSize=24,
                            fontWeight="bold",
                            color="#333",
                            anchor="middle"  # cƒÉn gi·ªØa
                        )
                    )
                )

                cols[i % 2].altair_chart(chart, use_container_width=True)

                if i % 2 == 1 and i < len(metrics_list) - 1:
                    cols = st.columns(2)


if __name__ == "__main__":
    main()
